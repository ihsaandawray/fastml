{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99379a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb048285",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "words = response = requests.get(url).text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5eb4d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['successfully',\n",
       " 'those',\n",
       " 'catalogs',\n",
       " 'layers',\n",
       " 'outlet',\n",
       " 'fall',\n",
       " 'usually',\n",
       " 'crimes',\n",
       " 'peer',\n",
       " 'checks']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352cfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5575eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e18b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36484d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in b.items():\n",
    "    s1, s2 = k\n",
    "    i1 = stoi[s1]\n",
    "    i2 = stoi[s2]\n",
    "    N[i1, i2] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3972cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 0.01).float()\n",
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69827f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "guma\n",
      "haplolotlesise\n",
      "prst\n",
      "c\n",
      "ba\n",
      "mangejalet\n",
      "suacurid\n",
      "ucrd\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  \n",
    "  out = []\n",
    "  idx = 0\n",
    "  while True:\n",
    "    p = P[idx]\n",
    "    idx = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "    if idx == 0:\n",
    "      break\n",
    "    out.append(itos[idx])\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1910238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-189780.3594)\n",
      "nll=tensor(189780.3594)\n",
      "2.501059055328369\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    prob = P[ix1, ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbbac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae73d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "num_classes = 27  \n",
    "embedding_dim = 10  \n",
    "\n",
    "\n",
    "class TrigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrigramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        # [batch_size, 2, embedding_dim]\n",
    "        flat = embedded.view(-1, embedding_dim * 2)\n",
    "        logits = self.fc(flat)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031e0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "model = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a7013fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 2.432063341140747\n",
      "Epoch 1, Batch 5, Loss: 2.1590428352355957\n",
      "Epoch 1, Batch 10, Loss: 2.284173011779785\n",
      "Epoch 1, Batch 15, Loss: 2.2679643630981445\n",
      "Epoch 1, Batch 20, Loss: 2.2808666229248047\n",
      "Epoch 1, Batch 25, Loss: 2.274916172027588\n",
      "Epoch 1, Batch 30, Loss: 2.3293917179107666\n",
      "Epoch 1, Batch 35, Loss: 2.281251907348633\n",
      "Epoch 1, Batch 40, Loss: 2.140366315841675\n",
      "Epoch 1, Batch 45, Loss: 2.3074188232421875\n",
      "Epoch 1, Batch 50, Loss: 2.485408067703247\n",
      "Epoch 1, Batch 55, Loss: 2.4060025215148926\n",
      "Epoch 1, Batch 60, Loss: 2.420342445373535\n",
      "Epoch 1, Batch 65, Loss: 2.6857898235321045\n",
      "Epoch 1, Batch 70, Loss: 2.3474466800689697\n",
      "Epoch 1, Batch 75, Loss: 2.277700901031494\n",
      "Epoch 1, Batch 80, Loss: 2.2970311641693115\n",
      "Epoch 1, Batch 85, Loss: 2.3025431632995605\n",
      "Epoch 1, Batch 90, Loss: 2.466430187225342\n",
      "Epoch 1, Batch 95, Loss: 2.5394506454467773\n",
      "Epoch 1, Batch 100, Loss: 2.470600128173828\n",
      "Epoch 1, Batch 105, Loss: 2.3043363094329834\n",
      "Epoch 1, Batch 110, Loss: 2.3121018409729004\n",
      "Epoch 1, Batch 115, Loss: 2.3942067623138428\n",
      "Epoch 1, Batch 120, Loss: 2.3314502239227295\n",
      "Epoch 1, Batch 125, Loss: 2.3522133827209473\n",
      "Epoch 1, Batch 130, Loss: 2.3342325687408447\n",
      "Epoch 1, Batch 135, Loss: 2.243119478225708\n",
      "Epoch 1, Batch 140, Loss: 2.274165391921997\n",
      "Epoch 1, Batch 145, Loss: 2.2646689414978027\n",
      "Epoch 1, Batch 150, Loss: 2.55859375\n",
      "Epoch 1, Batch 155, Loss: 2.3441529273986816\n",
      "Epoch 1, Batch 160, Loss: 2.540635824203491\n",
      "Epoch 1, Batch 165, Loss: 2.085923910140991\n",
      "Epoch 1, Batch 170, Loss: 2.34837007522583\n",
      "Epoch 1, Batch 175, Loss: 2.5964765548706055\n",
      "Epoch 1, Batch 180, Loss: 2.2726387977600098\n",
      "Epoch 1, Batch 185, Loss: 2.4485158920288086\n",
      "Epoch 1, Batch 190, Loss: 2.318161964416504\n",
      "Epoch 1, Batch 195, Loss: 2.4128975868225098\n",
      "Epoch 1, Batch 200, Loss: 2.3620574474334717\n",
      "Epoch 1, Batch 205, Loss: 2.317586898803711\n",
      "Epoch 1, Batch 210, Loss: 2.4198262691497803\n",
      "Epoch 1, Batch 215, Loss: 2.3935976028442383\n",
      "Epoch 1, Batch 220, Loss: 2.2791738510131836\n",
      "Epoch 1, Batch 225, Loss: 2.271972894668579\n",
      "Epoch 1, Batch 230, Loss: 2.2648727893829346\n",
      "Epoch 1, Batch 235, Loss: 2.358880043029785\n",
      "Epoch 1, Batch 240, Loss: 2.406567335128784\n",
      "Epoch 1, Batch 245, Loss: 2.2926418781280518\n",
      "Epoch 1, Batch 250, Loss: 2.4148521423339844\n",
      "Epoch 1, Batch 255, Loss: 2.4672701358795166\n",
      "Epoch 1, Batch 260, Loss: 2.2709641456604004\n",
      "Epoch 1, Batch 265, Loss: 2.4223690032958984\n",
      "Epoch 1, Batch 270, Loss: 2.365736961364746\n",
      "Epoch 1, Batch 275, Loss: 2.263554573059082\n",
      "Epoch 1, Batch 280, Loss: 2.2226152420043945\n",
      "Epoch 1, Batch 285, Loss: 2.294616460800171\n",
      "Epoch 1, Batch 290, Loss: 2.1831672191619873\n",
      "Epoch 1, Batch 295, Loss: 2.2946159839630127\n",
      "Epoch 1, Batch 300, Loss: 2.2499678134918213\n",
      "Epoch 1, Batch 305, Loss: 2.13407039642334\n",
      "Epoch 1, Batch 310, Loss: 2.373008966445923\n",
      "Epoch 1, Batch 315, Loss: 2.216273784637451\n",
      "Epoch 1, Batch 320, Loss: 2.320964813232422\n",
      "Epoch 1, Batch 325, Loss: 2.4658052921295166\n",
      "Epoch 1, Batch 330, Loss: 2.291046380996704\n",
      "Epoch 1, Batch 335, Loss: 2.5881853103637695\n",
      "Epoch 1, Batch 340, Loss: 2.2882840633392334\n",
      "Epoch 1, Batch 345, Loss: 2.5808403491973877\n",
      "Epoch 1, Batch 350, Loss: 2.322406053543091\n",
      "Epoch 1, Batch 355, Loss: 2.6009552478790283\n",
      "Epoch 1, Batch 360, Loss: 2.370412588119507\n",
      "Epoch 1, Batch 365, Loss: 2.2172253131866455\n",
      "Epoch 1, Batch 370, Loss: 2.5469632148742676\n",
      "Epoch 1, Batch 375, Loss: 2.156989336013794\n",
      "Epoch 1, Batch 380, Loss: 2.4348227977752686\n",
      "Epoch 1, Batch 385, Loss: 2.3491361141204834\n",
      "Epoch 1, Batch 390, Loss: 2.355024814605713\n",
      "Epoch 1, Batch 395, Loss: 2.340944766998291\n",
      "Epoch 1, Batch 400, Loss: 2.410490036010742\n",
      "Epoch 1, Batch 405, Loss: 2.3842687606811523\n",
      "Epoch 1, Batch 410, Loss: 2.4718592166900635\n",
      "Epoch 1, Batch 415, Loss: 2.497673749923706\n",
      "Epoch 1, Batch 420, Loss: 2.320939540863037\n",
      "Epoch 1, Batch 425, Loss: 2.388723611831665\n",
      "Epoch 1, Batch 430, Loss: 2.3272809982299805\n",
      "Epoch 1, Batch 435, Loss: 2.5516271591186523\n",
      "Epoch 1, Batch 440, Loss: 2.3155677318573\n",
      "Epoch 1, Batch 445, Loss: 2.2406985759735107\n",
      "Epoch 1, Batch 450, Loss: 2.397120714187622\n",
      "Epoch 1, Batch 455, Loss: 2.37320876121521\n",
      "Epoch 1, Batch 460, Loss: 2.5293684005737305\n",
      "Epoch 1, Batch 465, Loss: 2.2026286125183105\n",
      "Epoch 1, Batch 470, Loss: 2.401825189590454\n",
      "Epoch 1, Batch 475, Loss: 2.272101402282715\n",
      "Epoch 1, Batch 480, Loss: 2.219866991043091\n",
      "Epoch 1, Batch 485, Loss: 2.468543291091919\n",
      "Epoch 1, Batch 490, Loss: 2.1706366539001465\n",
      "Epoch 1, Batch 495, Loss: 2.351191520690918\n",
      "Epoch 1, Batch 500, Loss: 2.402299642562866\n",
      "Epoch 1, Batch 505, Loss: 2.328015089035034\n",
      "Epoch 1, Batch 510, Loss: 2.362480401992798\n",
      "Epoch 1, Batch 515, Loss: 2.5550310611724854\n",
      "Epoch 1, Batch 520, Loss: 2.530864953994751\n",
      "Epoch 1, Batch 525, Loss: 2.2532646656036377\n",
      "Epoch 1, Batch 530, Loss: 2.328788995742798\n",
      "Epoch 1, Batch 535, Loss: 2.283506393432617\n",
      "Epoch 1, Batch 540, Loss: 2.2603163719177246\n",
      "Epoch 1, Batch 545, Loss: 2.4217259883880615\n",
      "Epoch 1, Batch 550, Loss: 2.2021701335906982\n",
      "Epoch 1, Batch 555, Loss: 2.3434717655181885\n",
      "Epoch 1, Batch 560, Loss: 2.259254217147827\n",
      "Epoch 1, Batch 565, Loss: 2.1663992404937744\n",
      "Epoch 1, Batch 570, Loss: 2.195725440979004\n",
      "Epoch 1, Batch 575, Loss: 2.654689073562622\n",
      "Epoch 1, Batch 580, Loss: 2.4568328857421875\n",
      "Epoch 1, Batch 585, Loss: 2.242863178253174\n",
      "Epoch 1, Batch 590, Loss: 2.3640079498291016\n",
      "Epoch 1, Batch 595, Loss: 2.274141788482666\n",
      "Epoch 1, Batch 600, Loss: 2.3152108192443848\n",
      "Epoch 1, Batch 605, Loss: 2.48720121383667\n",
      "Epoch 1, Batch 610, Loss: 2.3594210147857666\n",
      "Epoch 1, Batch 615, Loss: 2.3563225269317627\n",
      "Epoch 1, Batch 620, Loss: 2.3094825744628906\n",
      "Epoch 1, Batch 625, Loss: 2.418881893157959\n",
      "Epoch 1, Batch 630, Loss: 2.191702365875244\n",
      "Epoch 1, Batch 635, Loss: 2.2114417552948\n",
      "Epoch 1, Batch 640, Loss: 2.474956750869751\n",
      "Epoch 1, Batch 645, Loss: 2.547696590423584\n",
      "Epoch 1, Batch 650, Loss: 2.306852340698242\n",
      "Epoch 1, Batch 655, Loss: 2.3976094722747803\n",
      "Epoch 2, Batch 0, Loss: 2.2682108879089355\n",
      "Epoch 2, Batch 5, Loss: 2.313798427581787\n",
      "Epoch 2, Batch 10, Loss: 2.28690242767334\n",
      "Epoch 2, Batch 15, Loss: 2.291475772857666\n",
      "Epoch 2, Batch 20, Loss: 2.4025120735168457\n",
      "Epoch 2, Batch 25, Loss: 2.1442008018493652\n",
      "Epoch 2, Batch 30, Loss: 2.179347276687622\n",
      "Epoch 2, Batch 35, Loss: 2.175257444381714\n",
      "Epoch 2, Batch 40, Loss: 2.3674330711364746\n",
      "Epoch 2, Batch 45, Loss: 2.210569381713867\n",
      "Epoch 2, Batch 50, Loss: 2.444990634918213\n",
      "Epoch 2, Batch 55, Loss: 2.200575828552246\n",
      "Epoch 2, Batch 60, Loss: 2.306870222091675\n",
      "Epoch 2, Batch 65, Loss: 2.2506377696990967\n",
      "Epoch 2, Batch 70, Loss: 2.316702127456665\n",
      "Epoch 2, Batch 75, Loss: 2.371182918548584\n",
      "Epoch 2, Batch 80, Loss: 2.3914923667907715\n",
      "Epoch 2, Batch 85, Loss: 2.4844348430633545\n",
      "Epoch 2, Batch 90, Loss: 2.4251973628997803\n",
      "Epoch 2, Batch 95, Loss: 2.2672815322875977\n",
      "Epoch 2, Batch 100, Loss: 2.2457845211029053\n",
      "Epoch 2, Batch 105, Loss: 2.3188064098358154\n",
      "Epoch 2, Batch 110, Loss: 2.2883009910583496\n",
      "Epoch 2, Batch 115, Loss: 2.37105655670166\n",
      "Epoch 2, Batch 120, Loss: 2.2966561317443848\n",
      "Epoch 2, Batch 125, Loss: 2.338822841644287\n",
      "Epoch 2, Batch 130, Loss: 2.2949838638305664\n",
      "Epoch 2, Batch 135, Loss: 2.373929023742676\n",
      "Epoch 2, Batch 140, Loss: 2.458836317062378\n",
      "Epoch 2, Batch 145, Loss: 2.2723190784454346\n",
      "Epoch 2, Batch 150, Loss: 2.4543423652648926\n",
      "Epoch 2, Batch 155, Loss: 2.3674709796905518\n",
      "Epoch 2, Batch 160, Loss: 2.1817452907562256\n",
      "Epoch 2, Batch 165, Loss: 2.2770867347717285\n",
      "Epoch 2, Batch 170, Loss: 2.1836752891540527\n",
      "Epoch 2, Batch 175, Loss: 2.3223466873168945\n",
      "Epoch 2, Batch 180, Loss: 2.4505701065063477\n",
      "Epoch 2, Batch 185, Loss: 2.407163143157959\n",
      "Epoch 2, Batch 190, Loss: 2.2505643367767334\n",
      "Epoch 2, Batch 195, Loss: 2.2430732250213623\n",
      "Epoch 2, Batch 200, Loss: 2.443103075027466\n",
      "Epoch 2, Batch 205, Loss: 2.4757747650146484\n",
      "Epoch 2, Batch 210, Loss: 2.290081739425659\n",
      "Epoch 2, Batch 215, Loss: 2.367884874343872\n",
      "Epoch 2, Batch 220, Loss: 2.4028093814849854\n",
      "Epoch 2, Batch 225, Loss: 2.426809072494507\n",
      "Epoch 2, Batch 230, Loss: 2.288797378540039\n",
      "Epoch 2, Batch 235, Loss: 2.291429042816162\n",
      "Epoch 2, Batch 240, Loss: 2.3483541011810303\n",
      "Epoch 2, Batch 245, Loss: 2.422563076019287\n",
      "Epoch 2, Batch 250, Loss: 2.223987579345703\n",
      "Epoch 2, Batch 255, Loss: 2.419560432434082\n",
      "Epoch 2, Batch 260, Loss: 2.3777377605438232\n",
      "Epoch 2, Batch 265, Loss: 2.3887529373168945\n",
      "Epoch 2, Batch 270, Loss: 2.4733710289001465\n",
      "Epoch 2, Batch 275, Loss: 2.3210747241973877\n",
      "Epoch 2, Batch 280, Loss: 2.284879684448242\n",
      "Epoch 2, Batch 285, Loss: 2.3718976974487305\n",
      "Epoch 2, Batch 290, Loss: 2.4274327754974365\n",
      "Epoch 2, Batch 295, Loss: 2.2552645206451416\n",
      "Epoch 2, Batch 300, Loss: 2.2053892612457275\n",
      "Epoch 2, Batch 305, Loss: 2.254425525665283\n",
      "Epoch 2, Batch 310, Loss: 2.3129384517669678\n",
      "Epoch 2, Batch 315, Loss: 2.1660337448120117\n",
      "Epoch 2, Batch 320, Loss: 2.488363027572632\n",
      "Epoch 2, Batch 325, Loss: 2.5116875171661377\n",
      "Epoch 2, Batch 330, Loss: 2.21085786819458\n",
      "Epoch 2, Batch 335, Loss: 2.3385653495788574\n",
      "Epoch 2, Batch 340, Loss: 2.306262731552124\n",
      "Epoch 2, Batch 345, Loss: 2.395402669906616\n",
      "Epoch 2, Batch 350, Loss: 2.3079655170440674\n",
      "Epoch 2, Batch 355, Loss: 2.4232451915740967\n",
      "Epoch 2, Batch 360, Loss: 2.589306592941284\n",
      "Epoch 2, Batch 365, Loss: 2.195789337158203\n",
      "Epoch 2, Batch 370, Loss: 2.405111789703369\n",
      "Epoch 2, Batch 375, Loss: 2.3920857906341553\n",
      "Epoch 2, Batch 380, Loss: 2.4680206775665283\n",
      "Epoch 2, Batch 385, Loss: 2.389131784439087\n",
      "Epoch 2, Batch 390, Loss: 2.3122267723083496\n",
      "Epoch 2, Batch 395, Loss: 2.4628052711486816\n",
      "Epoch 2, Batch 400, Loss: 2.2254624366760254\n",
      "Epoch 2, Batch 405, Loss: 2.4198191165924072\n",
      "Epoch 2, Batch 410, Loss: 2.317950963973999\n",
      "Epoch 2, Batch 415, Loss: 2.4713551998138428\n",
      "Epoch 2, Batch 420, Loss: 2.295574903488159\n",
      "Epoch 2, Batch 425, Loss: 2.332465410232544\n",
      "Epoch 2, Batch 430, Loss: 2.409060478210449\n",
      "Epoch 2, Batch 435, Loss: 2.5433332920074463\n",
      "Epoch 2, Batch 440, Loss: 2.375270128250122\n",
      "Epoch 2, Batch 445, Loss: 2.214625597000122\n",
      "Epoch 2, Batch 450, Loss: 2.6360855102539062\n",
      "Epoch 2, Batch 455, Loss: 2.2073371410369873\n",
      "Epoch 2, Batch 460, Loss: 2.4960286617279053\n",
      "Epoch 2, Batch 465, Loss: 2.0861189365386963\n",
      "Epoch 2, Batch 470, Loss: 2.673147201538086\n",
      "Epoch 2, Batch 475, Loss: 2.5082404613494873\n",
      "Epoch 2, Batch 480, Loss: 2.4250617027282715\n",
      "Epoch 2, Batch 485, Loss: 2.2199087142944336\n",
      "Epoch 2, Batch 490, Loss: 2.4540092945098877\n",
      "Epoch 2, Batch 495, Loss: 2.3519392013549805\n",
      "Epoch 2, Batch 500, Loss: 2.4626517295837402\n",
      "Epoch 2, Batch 505, Loss: 2.2274372577667236\n",
      "Epoch 2, Batch 510, Loss: 2.282675266265869\n",
      "Epoch 2, Batch 515, Loss: 2.4474809169769287\n",
      "Epoch 2, Batch 520, Loss: 2.3184406757354736\n",
      "Epoch 2, Batch 525, Loss: 2.4167537689208984\n",
      "Epoch 2, Batch 530, Loss: 2.469179153442383\n",
      "Epoch 2, Batch 535, Loss: 2.4483280181884766\n",
      "Epoch 2, Batch 540, Loss: 2.2577826976776123\n",
      "Epoch 2, Batch 545, Loss: 2.556975841522217\n",
      "Epoch 2, Batch 550, Loss: 2.4044666290283203\n",
      "Epoch 2, Batch 555, Loss: 2.362814426422119\n",
      "Epoch 2, Batch 560, Loss: 2.3025238513946533\n",
      "Epoch 2, Batch 565, Loss: 2.315511465072632\n",
      "Epoch 2, Batch 570, Loss: 2.4600372314453125\n",
      "Epoch 2, Batch 575, Loss: 2.457594394683838\n",
      "Epoch 2, Batch 580, Loss: 2.1786258220672607\n",
      "Epoch 2, Batch 585, Loss: 2.2944748401641846\n",
      "Epoch 2, Batch 590, Loss: 2.3480238914489746\n",
      "Epoch 2, Batch 595, Loss: 2.1972391605377197\n",
      "Epoch 2, Batch 600, Loss: 2.4083471298217773\n",
      "Epoch 2, Batch 605, Loss: 2.295574903488159\n",
      "Epoch 2, Batch 610, Loss: 2.3420803546905518\n",
      "Epoch 2, Batch 615, Loss: 2.3895087242126465\n",
      "Epoch 2, Batch 620, Loss: 2.104377508163452\n",
      "Epoch 2, Batch 625, Loss: 2.658024311065674\n",
      "Epoch 2, Batch 630, Loss: 2.22243070602417\n",
      "Epoch 2, Batch 635, Loss: 2.423652410507202\n",
      "Epoch 2, Batch 640, Loss: 2.3411595821380615\n",
      "Epoch 2, Batch 645, Loss: 2.3705456256866455\n",
      "Epoch 2, Batch 650, Loss: 2.3646857738494873\n",
      "Epoch 2, Batch 655, Loss: 2.225027084350586\n",
      "Epoch 3, Batch 0, Loss: 2.420038938522339\n",
      "Epoch 3, Batch 5, Loss: 2.430295467376709\n",
      "Epoch 3, Batch 10, Loss: 2.4277400970458984\n",
      "Epoch 3, Batch 15, Loss: 2.3769962787628174\n",
      "Epoch 3, Batch 20, Loss: 2.3115792274475098\n",
      "Epoch 3, Batch 25, Loss: 2.402878999710083\n",
      "Epoch 3, Batch 30, Loss: 2.365365505218506\n",
      "Epoch 3, Batch 35, Loss: 2.4931540489196777\n",
      "Epoch 3, Batch 40, Loss: 2.5046591758728027\n",
      "Epoch 3, Batch 45, Loss: 2.4286694526672363\n",
      "Epoch 3, Batch 50, Loss: 2.284914970397949\n",
      "Epoch 3, Batch 55, Loss: 2.366595983505249\n",
      "Epoch 3, Batch 60, Loss: 2.212693214416504\n",
      "Epoch 3, Batch 65, Loss: 2.296942949295044\n",
      "Epoch 3, Batch 70, Loss: 2.412214756011963\n",
      "Epoch 3, Batch 75, Loss: 2.66666316986084\n",
      "Epoch 3, Batch 80, Loss: 2.4960649013519287\n",
      "Epoch 3, Batch 85, Loss: 2.393805980682373\n",
      "Epoch 3, Batch 90, Loss: 2.3415780067443848\n",
      "Epoch 3, Batch 95, Loss: 2.2404279708862305\n",
      "Epoch 3, Batch 100, Loss: 2.434727430343628\n",
      "Epoch 3, Batch 105, Loss: 2.420750617980957\n",
      "Epoch 3, Batch 110, Loss: 2.205681800842285\n",
      "Epoch 3, Batch 115, Loss: 2.3005881309509277\n",
      "Epoch 3, Batch 120, Loss: 2.4408273696899414\n",
      "Epoch 3, Batch 125, Loss: 2.4495723247528076\n",
      "Epoch 3, Batch 130, Loss: 2.351449489593506\n",
      "Epoch 3, Batch 135, Loss: 2.3153133392333984\n",
      "Epoch 3, Batch 140, Loss: 2.257124662399292\n",
      "Epoch 3, Batch 145, Loss: 2.2928643226623535\n",
      "Epoch 3, Batch 150, Loss: 2.424813747406006\n",
      "Epoch 3, Batch 155, Loss: 2.130084276199341\n",
      "Epoch 3, Batch 160, Loss: 2.478578805923462\n",
      "Epoch 3, Batch 165, Loss: 2.4858615398406982\n",
      "Epoch 3, Batch 170, Loss: 2.3079023361206055\n",
      "Epoch 3, Batch 175, Loss: 2.3830926418304443\n",
      "Epoch 3, Batch 180, Loss: 2.5786914825439453\n",
      "Epoch 3, Batch 185, Loss: 2.4609813690185547\n",
      "Epoch 3, Batch 190, Loss: 2.2149477005004883\n",
      "Epoch 3, Batch 195, Loss: 2.2156357765197754\n",
      "Epoch 3, Batch 200, Loss: 2.1116368770599365\n",
      "Epoch 3, Batch 205, Loss: 2.413022518157959\n",
      "Epoch 3, Batch 210, Loss: 2.3789076805114746\n",
      "Epoch 3, Batch 215, Loss: 2.4149010181427\n",
      "Epoch 3, Batch 220, Loss: 2.350534439086914\n",
      "Epoch 3, Batch 225, Loss: 2.509091854095459\n",
      "Epoch 3, Batch 230, Loss: 2.5323853492736816\n",
      "Epoch 3, Batch 235, Loss: 2.385928153991699\n",
      "Epoch 3, Batch 240, Loss: 2.5043022632598877\n",
      "Epoch 3, Batch 245, Loss: 2.2110471725463867\n",
      "Epoch 3, Batch 250, Loss: 2.5336267948150635\n",
      "Epoch 3, Batch 255, Loss: 2.556595802307129\n",
      "Epoch 3, Batch 260, Loss: 2.239971399307251\n",
      "Epoch 3, Batch 265, Loss: 2.1978237628936768\n",
      "Epoch 3, Batch 270, Loss: 2.3949697017669678\n",
      "Epoch 3, Batch 275, Loss: 2.5512378215789795\n",
      "Epoch 3, Batch 280, Loss: 2.1715199947357178\n",
      "Epoch 3, Batch 285, Loss: 2.3858275413513184\n",
      "Epoch 3, Batch 290, Loss: 2.4406185150146484\n",
      "Epoch 3, Batch 295, Loss: 2.3712587356567383\n",
      "Epoch 3, Batch 300, Loss: 2.4400010108947754\n",
      "Epoch 3, Batch 305, Loss: 2.3079023361206055\n",
      "Epoch 3, Batch 310, Loss: 2.3988332748413086\n",
      "Epoch 3, Batch 315, Loss: 2.174030303955078\n",
      "Epoch 3, Batch 320, Loss: 2.2644495964050293\n",
      "Epoch 3, Batch 325, Loss: 2.2916481494903564\n",
      "Epoch 3, Batch 330, Loss: 2.255946159362793\n",
      "Epoch 3, Batch 335, Loss: 2.2549989223480225\n",
      "Epoch 3, Batch 340, Loss: 2.4430441856384277\n",
      "Epoch 3, Batch 345, Loss: 2.2772226333618164\n",
      "Epoch 3, Batch 350, Loss: 2.467379570007324\n",
      "Epoch 3, Batch 355, Loss: 2.2486047744750977\n",
      "Epoch 3, Batch 360, Loss: 2.146923542022705\n",
      "Epoch 3, Batch 365, Loss: 2.2853662967681885\n",
      "Epoch 3, Batch 370, Loss: 2.3028502464294434\n",
      "Epoch 3, Batch 375, Loss: 2.2776947021484375\n",
      "Epoch 3, Batch 380, Loss: 2.307164192199707\n",
      "Epoch 3, Batch 385, Loss: 2.381004810333252\n",
      "Epoch 3, Batch 390, Loss: 2.306673288345337\n",
      "Epoch 3, Batch 395, Loss: 2.2492921352386475\n",
      "Epoch 3, Batch 400, Loss: 2.3761115074157715\n",
      "Epoch 3, Batch 405, Loss: 2.3284692764282227\n",
      "Epoch 3, Batch 410, Loss: 2.444866180419922\n",
      "Epoch 3, Batch 415, Loss: 2.4032797813415527\n",
      "Epoch 3, Batch 420, Loss: 2.515805244445801\n",
      "Epoch 3, Batch 425, Loss: 2.5301692485809326\n",
      "Epoch 3, Batch 430, Loss: 2.125943183898926\n",
      "Epoch 3, Batch 435, Loss: 2.295560359954834\n",
      "Epoch 3, Batch 440, Loss: 2.4639029502868652\n",
      "Epoch 3, Batch 445, Loss: 2.2099907398223877\n",
      "Epoch 3, Batch 450, Loss: 2.3453149795532227\n",
      "Epoch 3, Batch 455, Loss: 2.3435213565826416\n",
      "Epoch 3, Batch 460, Loss: 2.465393543243408\n",
      "Epoch 3, Batch 465, Loss: 2.3502445220947266\n",
      "Epoch 3, Batch 470, Loss: 2.5608811378479004\n",
      "Epoch 3, Batch 475, Loss: 2.213461399078369\n",
      "Epoch 3, Batch 480, Loss: 2.2972397804260254\n",
      "Epoch 3, Batch 485, Loss: 2.4461426734924316\n",
      "Epoch 3, Batch 490, Loss: 2.349198579788208\n",
      "Epoch 3, Batch 495, Loss: 2.2709145545959473\n",
      "Epoch 3, Batch 500, Loss: 2.3001036643981934\n",
      "Epoch 3, Batch 505, Loss: 2.2460412979125977\n",
      "Epoch 3, Batch 510, Loss: 2.4439992904663086\n",
      "Epoch 3, Batch 515, Loss: 2.292595386505127\n",
      "Epoch 3, Batch 520, Loss: 2.428818941116333\n",
      "Epoch 3, Batch 525, Loss: 2.295208692550659\n",
      "Epoch 3, Batch 530, Loss: 2.457606554031372\n",
      "Epoch 3, Batch 535, Loss: 2.4870264530181885\n",
      "Epoch 3, Batch 540, Loss: 2.377017021179199\n",
      "Epoch 3, Batch 545, Loss: 2.3148250579833984\n",
      "Epoch 3, Batch 550, Loss: 2.3072824478149414\n",
      "Epoch 3, Batch 555, Loss: 2.6423375606536865\n",
      "Epoch 3, Batch 560, Loss: 2.4371497631073\n",
      "Epoch 3, Batch 565, Loss: 2.443319082260132\n",
      "Epoch 3, Batch 570, Loss: 2.309281826019287\n",
      "Epoch 3, Batch 575, Loss: 2.393948554992676\n",
      "Epoch 3, Batch 580, Loss: 2.5233922004699707\n",
      "Epoch 3, Batch 585, Loss: 2.2483315467834473\n",
      "Epoch 3, Batch 590, Loss: 2.5484073162078857\n",
      "Epoch 3, Batch 595, Loss: 2.3640079498291016\n",
      "Epoch 3, Batch 600, Loss: 2.1978418827056885\n",
      "Epoch 3, Batch 605, Loss: 2.4471845626831055\n",
      "Epoch 3, Batch 610, Loss: 2.4015684127807617\n",
      "Epoch 3, Batch 615, Loss: 2.0833141803741455\n",
      "Epoch 3, Batch 620, Loss: 2.353675603866577\n",
      "Epoch 3, Batch 625, Loss: 2.225714683532715\n",
      "Epoch 3, Batch 630, Loss: 2.34995436668396\n",
      "Epoch 3, Batch 635, Loss: 2.3765008449554443\n",
      "Epoch 3, Batch 640, Loss: 2.2954061031341553\n",
      "Epoch 3, Batch 645, Loss: 2.35432767868042\n",
      "Epoch 3, Batch 650, Loss: 2.5191097259521484\n",
      "Epoch 3, Batch 655, Loss: 2.206587314605713\n",
      "Epoch 4, Batch 0, Loss: 2.2961137294769287\n",
      "Epoch 4, Batch 5, Loss: 2.4713940620422363\n",
      "Epoch 4, Batch 10, Loss: 2.2386302947998047\n",
      "Epoch 4, Batch 15, Loss: 2.4947142601013184\n",
      "Epoch 4, Batch 20, Loss: 2.356448173522949\n",
      "Epoch 4, Batch 25, Loss: 2.198183059692383\n",
      "Epoch 4, Batch 30, Loss: 2.429799795150757\n",
      "Epoch 4, Batch 35, Loss: 2.3326518535614014\n",
      "Epoch 4, Batch 40, Loss: 2.3558523654937744\n",
      "Epoch 4, Batch 45, Loss: 2.3794984817504883\n",
      "Epoch 4, Batch 50, Loss: 2.2932775020599365\n",
      "Epoch 4, Batch 55, Loss: 2.2968430519104004\n",
      "Epoch 4, Batch 60, Loss: 2.210533380508423\n",
      "Epoch 4, Batch 65, Loss: 2.1218676567077637\n",
      "Epoch 4, Batch 70, Loss: 2.589048385620117\n",
      "Epoch 4, Batch 75, Loss: 2.143118143081665\n",
      "Epoch 4, Batch 80, Loss: 2.1864571571350098\n",
      "Epoch 4, Batch 85, Loss: 2.271070957183838\n",
      "Epoch 4, Batch 90, Loss: 2.3808786869049072\n",
      "Epoch 4, Batch 95, Loss: 2.576780319213867\n",
      "Epoch 4, Batch 100, Loss: 2.3927481174468994\n",
      "Epoch 4, Batch 105, Loss: 2.5290884971618652\n",
      "Epoch 4, Batch 110, Loss: 2.338831663131714\n",
      "Epoch 4, Batch 115, Loss: 2.1595215797424316\n",
      "Epoch 4, Batch 120, Loss: 2.284547805786133\n",
      "Epoch 4, Batch 125, Loss: 2.3602287769317627\n",
      "Epoch 4, Batch 130, Loss: 2.5107626914978027\n",
      "Epoch 4, Batch 135, Loss: 2.318722724914551\n",
      "Epoch 4, Batch 140, Loss: 2.522170066833496\n",
      "Epoch 4, Batch 145, Loss: 2.340987205505371\n",
      "Epoch 4, Batch 150, Loss: 2.4359030723571777\n",
      "Epoch 4, Batch 155, Loss: 2.340883255004883\n",
      "Epoch 4, Batch 160, Loss: 2.3670427799224854\n",
      "Epoch 4, Batch 165, Loss: 2.273689031600952\n",
      "Epoch 4, Batch 170, Loss: 2.2635304927825928\n",
      "Epoch 4, Batch 175, Loss: 2.0694665908813477\n",
      "Epoch 4, Batch 180, Loss: 2.2436001300811768\n",
      "Epoch 4, Batch 185, Loss: 2.4843387603759766\n",
      "Epoch 4, Batch 190, Loss: 2.506575584411621\n",
      "Epoch 4, Batch 195, Loss: 2.472782850265503\n",
      "Epoch 4, Batch 200, Loss: 2.2493011951446533\n",
      "Epoch 4, Batch 205, Loss: 2.317033290863037\n",
      "Epoch 4, Batch 210, Loss: 2.3462321758270264\n",
      "Epoch 4, Batch 215, Loss: 2.3933568000793457\n",
      "Epoch 4, Batch 220, Loss: 2.1041648387908936\n",
      "Epoch 4, Batch 225, Loss: 2.3792061805725098\n",
      "Epoch 4, Batch 230, Loss: 2.548537015914917\n",
      "Epoch 4, Batch 235, Loss: 2.539496898651123\n",
      "Epoch 4, Batch 240, Loss: 2.2288777828216553\n",
      "Epoch 4, Batch 245, Loss: 2.30595326423645\n",
      "Epoch 4, Batch 250, Loss: 2.1607463359832764\n",
      "Epoch 4, Batch 255, Loss: 2.1337387561798096\n",
      "Epoch 4, Batch 260, Loss: 2.5028257369995117\n",
      "Epoch 4, Batch 265, Loss: 2.142035961151123\n",
      "Epoch 4, Batch 270, Loss: 2.435955286026001\n",
      "Epoch 4, Batch 275, Loss: 2.344870090484619\n",
      "Epoch 4, Batch 280, Loss: 2.4498486518859863\n",
      "Epoch 4, Batch 285, Loss: 2.459768772125244\n",
      "Epoch 4, Batch 290, Loss: 2.5698015689849854\n",
      "Epoch 4, Batch 295, Loss: 2.30428409576416\n",
      "Epoch 4, Batch 300, Loss: 2.6284847259521484\n",
      "Epoch 4, Batch 305, Loss: 2.1214687824249268\n",
      "Epoch 4, Batch 310, Loss: 2.39200496673584\n",
      "Epoch 4, Batch 315, Loss: 2.272082567214966\n",
      "Epoch 4, Batch 320, Loss: 2.2609288692474365\n",
      "Epoch 4, Batch 325, Loss: 2.3837883472442627\n",
      "Epoch 4, Batch 330, Loss: 2.533205032348633\n",
      "Epoch 4, Batch 335, Loss: 2.446943998336792\n",
      "Epoch 4, Batch 340, Loss: 2.4331679344177246\n",
      "Epoch 4, Batch 345, Loss: 2.4063405990600586\n",
      "Epoch 4, Batch 350, Loss: 2.296290397644043\n",
      "Epoch 4, Batch 355, Loss: 2.4257612228393555\n",
      "Epoch 4, Batch 360, Loss: 2.4880473613739014\n",
      "Epoch 4, Batch 365, Loss: 2.3332314491271973\n",
      "Epoch 4, Batch 370, Loss: 2.433425188064575\n",
      "Epoch 4, Batch 375, Loss: 2.3444244861602783\n",
      "Epoch 4, Batch 380, Loss: 2.233168840408325\n",
      "Epoch 4, Batch 385, Loss: 2.42678165435791\n",
      "Epoch 4, Batch 390, Loss: 2.4825429916381836\n",
      "Epoch 4, Batch 395, Loss: 2.3494436740875244\n",
      "Epoch 4, Batch 400, Loss: 2.0949487686157227\n",
      "Epoch 4, Batch 405, Loss: 2.3416621685028076\n",
      "Epoch 4, Batch 410, Loss: 2.435598611831665\n",
      "Epoch 4, Batch 415, Loss: 2.392662286758423\n",
      "Epoch 4, Batch 420, Loss: 2.3511006832122803\n",
      "Epoch 4, Batch 425, Loss: 2.694394588470459\n",
      "Epoch 4, Batch 430, Loss: 2.5486385822296143\n",
      "Epoch 4, Batch 435, Loss: 2.6172659397125244\n",
      "Epoch 4, Batch 440, Loss: 2.4144093990325928\n",
      "Epoch 4, Batch 445, Loss: 2.4313459396362305\n",
      "Epoch 4, Batch 450, Loss: 2.521625518798828\n",
      "Epoch 4, Batch 455, Loss: 2.2275917530059814\n",
      "Epoch 4, Batch 460, Loss: 2.417653799057007\n",
      "Epoch 4, Batch 465, Loss: 2.1618974208831787\n",
      "Epoch 4, Batch 470, Loss: 2.1702816486358643\n",
      "Epoch 4, Batch 475, Loss: 2.393561601638794\n",
      "Epoch 4, Batch 480, Loss: 2.2200615406036377\n",
      "Epoch 4, Batch 485, Loss: 2.4075069427490234\n",
      "Epoch 4, Batch 490, Loss: 2.157675266265869\n",
      "Epoch 4, Batch 495, Loss: 2.242697238922119\n",
      "Epoch 4, Batch 500, Loss: 2.319242477416992\n",
      "Epoch 4, Batch 505, Loss: 2.4275641441345215\n",
      "Epoch 4, Batch 510, Loss: 2.457871437072754\n",
      "Epoch 4, Batch 515, Loss: 2.3239505290985107\n",
      "Epoch 4, Batch 520, Loss: 2.4273548126220703\n",
      "Epoch 4, Batch 525, Loss: 2.335376262664795\n",
      "Epoch 4, Batch 530, Loss: 2.260887384414673\n",
      "Epoch 4, Batch 535, Loss: 2.312757730484009\n",
      "Epoch 4, Batch 540, Loss: 2.2704808712005615\n",
      "Epoch 4, Batch 545, Loss: 2.5061452388763428\n",
      "Epoch 4, Batch 550, Loss: 2.2911038398742676\n",
      "Epoch 4, Batch 555, Loss: 2.2754459381103516\n",
      "Epoch 4, Batch 560, Loss: 2.5013606548309326\n",
      "Epoch 4, Batch 565, Loss: 2.223264217376709\n",
      "Epoch 4, Batch 570, Loss: 2.27751088142395\n",
      "Epoch 4, Batch 575, Loss: 2.3470983505249023\n",
      "Epoch 4, Batch 580, Loss: 2.3324966430664062\n",
      "Epoch 4, Batch 585, Loss: 2.477296829223633\n",
      "Epoch 4, Batch 590, Loss: 2.4798831939697266\n",
      "Epoch 4, Batch 595, Loss: 2.250870704650879\n",
      "Epoch 4, Batch 600, Loss: 2.1701951026916504\n",
      "Epoch 4, Batch 605, Loss: 2.2997841835021973\n",
      "Epoch 4, Batch 610, Loss: 2.4104163646698\n",
      "Epoch 4, Batch 615, Loss: 2.3802149295806885\n",
      "Epoch 4, Batch 620, Loss: 2.2494008541107178\n",
      "Epoch 4, Batch 625, Loss: 2.2639005184173584\n",
      "Epoch 4, Batch 630, Loss: 2.3373517990112305\n",
      "Epoch 4, Batch 635, Loss: 2.276524782180786\n",
      "Epoch 4, Batch 640, Loss: 2.591146945953369\n",
      "Epoch 4, Batch 645, Loss: 2.377424716949463\n",
      "Epoch 4, Batch 650, Loss: 2.305643081665039\n",
      "Epoch 4, Batch 655, Loss: 2.240307569503784\n",
      "Epoch 5, Batch 0, Loss: 2.3324036598205566\n",
      "Epoch 5, Batch 5, Loss: 2.298257827758789\n",
      "Epoch 5, Batch 10, Loss: 2.2960257530212402\n",
      "Epoch 5, Batch 15, Loss: 2.4525887966156006\n",
      "Epoch 5, Batch 20, Loss: 2.3517067432403564\n",
      "Epoch 5, Batch 25, Loss: 2.3363149166107178\n",
      "Epoch 5, Batch 30, Loss: 2.186079502105713\n",
      "Epoch 5, Batch 35, Loss: 2.3642454147338867\n",
      "Epoch 5, Batch 40, Loss: 2.2644622325897217\n",
      "Epoch 5, Batch 45, Loss: 2.237002372741699\n",
      "Epoch 5, Batch 50, Loss: 2.348123550415039\n",
      "Epoch 5, Batch 55, Loss: 2.5165185928344727\n",
      "Epoch 5, Batch 60, Loss: 2.303961753845215\n",
      "Epoch 5, Batch 65, Loss: 2.3778374195098877\n",
      "Epoch 5, Batch 70, Loss: 2.2105720043182373\n",
      "Epoch 5, Batch 75, Loss: 2.19307279586792\n",
      "Epoch 5, Batch 80, Loss: 2.3689000606536865\n",
      "Epoch 5, Batch 85, Loss: 2.564594030380249\n",
      "Epoch 5, Batch 90, Loss: 2.5482239723205566\n",
      "Epoch 5, Batch 95, Loss: 2.4902238845825195\n",
      "Epoch 5, Batch 100, Loss: 2.6650032997131348\n",
      "Epoch 5, Batch 105, Loss: 2.2395858764648438\n",
      "Epoch 5, Batch 110, Loss: 2.4679558277130127\n",
      "Epoch 5, Batch 115, Loss: 2.3542733192443848\n",
      "Epoch 5, Batch 120, Loss: 2.1884419918060303\n",
      "Epoch 5, Batch 125, Loss: 2.3017842769622803\n",
      "Epoch 5, Batch 130, Loss: 2.279876947402954\n",
      "Epoch 5, Batch 135, Loss: 2.370978355407715\n",
      "Epoch 5, Batch 140, Loss: 2.2732045650482178\n",
      "Epoch 5, Batch 145, Loss: 2.3719258308410645\n",
      "Epoch 5, Batch 150, Loss: 2.1954991817474365\n",
      "Epoch 5, Batch 155, Loss: 2.426281690597534\n",
      "Epoch 5, Batch 160, Loss: 2.216425895690918\n",
      "Epoch 5, Batch 165, Loss: 2.473954677581787\n",
      "Epoch 5, Batch 170, Loss: 2.3952651023864746\n",
      "Epoch 5, Batch 175, Loss: 2.3807477951049805\n",
      "Epoch 5, Batch 180, Loss: 2.213656425476074\n",
      "Epoch 5, Batch 185, Loss: 2.460853338241577\n",
      "Epoch 5, Batch 190, Loss: 2.363333225250244\n",
      "Epoch 5, Batch 195, Loss: 2.496073007583618\n",
      "Epoch 5, Batch 200, Loss: 2.281524896621704\n",
      "Epoch 5, Batch 205, Loss: 2.191194534301758\n",
      "Epoch 5, Batch 210, Loss: 2.269711971282959\n",
      "Epoch 5, Batch 215, Loss: 2.227278232574463\n",
      "Epoch 5, Batch 220, Loss: 2.2906196117401123\n",
      "Epoch 5, Batch 225, Loss: 2.270256757736206\n",
      "Epoch 5, Batch 230, Loss: 2.374389410018921\n",
      "Epoch 5, Batch 235, Loss: 2.2076728343963623\n",
      "Epoch 5, Batch 240, Loss: 2.3861300945281982\n",
      "Epoch 5, Batch 245, Loss: 2.347442388534546\n",
      "Epoch 5, Batch 250, Loss: 2.2906792163848877\n",
      "Epoch 5, Batch 255, Loss: 2.140878677368164\n",
      "Epoch 5, Batch 260, Loss: 2.4363105297088623\n",
      "Epoch 5, Batch 265, Loss: 2.5035669803619385\n",
      "Epoch 5, Batch 270, Loss: 2.4027228355407715\n",
      "Epoch 5, Batch 275, Loss: 2.3375329971313477\n",
      "Epoch 5, Batch 280, Loss: 2.4237897396087646\n",
      "Epoch 5, Batch 285, Loss: 2.390052556991577\n",
      "Epoch 5, Batch 290, Loss: 2.373394727706909\n",
      "Epoch 5, Batch 295, Loss: 2.3059093952178955\n",
      "Epoch 5, Batch 300, Loss: 2.3913965225219727\n",
      "Epoch 5, Batch 305, Loss: 2.371692180633545\n",
      "Epoch 5, Batch 310, Loss: 2.499819040298462\n",
      "Epoch 5, Batch 315, Loss: 2.3506650924682617\n",
      "Epoch 5, Batch 320, Loss: 2.4011435508728027\n",
      "Epoch 5, Batch 325, Loss: 2.2104947566986084\n",
      "Epoch 5, Batch 330, Loss: 2.2810163497924805\n",
      "Epoch 5, Batch 335, Loss: 2.430696964263916\n",
      "Epoch 5, Batch 340, Loss: 2.4307663440704346\n",
      "Epoch 5, Batch 345, Loss: 2.301961898803711\n",
      "Epoch 5, Batch 350, Loss: 2.319589138031006\n",
      "Epoch 5, Batch 355, Loss: 2.5324230194091797\n",
      "Epoch 5, Batch 360, Loss: 2.233954668045044\n",
      "Epoch 5, Batch 365, Loss: 2.236799955368042\n",
      "Epoch 5, Batch 370, Loss: 2.4287970066070557\n",
      "Epoch 5, Batch 375, Loss: 2.3660690784454346\n",
      "Epoch 5, Batch 380, Loss: 2.180516004562378\n",
      "Epoch 5, Batch 385, Loss: 2.090161085128784\n",
      "Epoch 5, Batch 390, Loss: 2.4486570358276367\n",
      "Epoch 5, Batch 395, Loss: 2.358215093612671\n",
      "Epoch 5, Batch 400, Loss: 2.2212085723876953\n",
      "Epoch 5, Batch 405, Loss: 2.2263805866241455\n",
      "Epoch 5, Batch 410, Loss: 2.3893754482269287\n",
      "Epoch 5, Batch 415, Loss: 2.375990152359009\n",
      "Epoch 5, Batch 420, Loss: 2.3010807037353516\n",
      "Epoch 5, Batch 425, Loss: 2.4978439807891846\n",
      "Epoch 5, Batch 430, Loss: 2.162376642227173\n",
      "Epoch 5, Batch 435, Loss: 2.374876022338867\n",
      "Epoch 5, Batch 440, Loss: 2.503009557723999\n",
      "Epoch 5, Batch 445, Loss: 2.1732864379882812\n",
      "Epoch 5, Batch 450, Loss: 2.521416664123535\n",
      "Epoch 5, Batch 455, Loss: 2.2898998260498047\n",
      "Epoch 5, Batch 460, Loss: 2.38907527923584\n",
      "Epoch 5, Batch 465, Loss: 2.268632650375366\n",
      "Epoch 5, Batch 470, Loss: 2.425661563873291\n",
      "Epoch 5, Batch 475, Loss: 2.5017242431640625\n",
      "Epoch 5, Batch 480, Loss: 2.2686991691589355\n",
      "Epoch 5, Batch 485, Loss: 2.309330940246582\n",
      "Epoch 5, Batch 490, Loss: 2.3108067512512207\n",
      "Epoch 5, Batch 495, Loss: 2.27726149559021\n",
      "Epoch 5, Batch 500, Loss: 2.1875200271606445\n",
      "Epoch 5, Batch 505, Loss: 2.3607606887817383\n",
      "Epoch 5, Batch 510, Loss: 2.3282132148742676\n",
      "Epoch 5, Batch 515, Loss: 2.350198745727539\n",
      "Epoch 5, Batch 520, Loss: 2.333402633666992\n",
      "Epoch 5, Batch 525, Loss: 2.4182350635528564\n",
      "Epoch 5, Batch 530, Loss: 2.5776989459991455\n",
      "Epoch 5, Batch 535, Loss: 2.403757333755493\n",
      "Epoch 5, Batch 540, Loss: 2.4808385372161865\n",
      "Epoch 5, Batch 545, Loss: 2.3242974281311035\n",
      "Epoch 5, Batch 550, Loss: 2.2672383785247803\n",
      "Epoch 5, Batch 555, Loss: 2.4146039485931396\n",
      "Epoch 5, Batch 560, Loss: 2.435586452484131\n",
      "Epoch 5, Batch 565, Loss: 2.4057934284210205\n",
      "Epoch 5, Batch 570, Loss: 2.214306116104126\n",
      "Epoch 5, Batch 575, Loss: 2.421320915222168\n",
      "Epoch 5, Batch 580, Loss: 2.4118266105651855\n",
      "Epoch 5, Batch 585, Loss: 2.455435037612915\n",
      "Epoch 5, Batch 590, Loss: 2.3852579593658447\n",
      "Epoch 5, Batch 595, Loss: 2.265861988067627\n",
      "Epoch 5, Batch 600, Loss: 2.467339277267456\n",
      "Epoch 5, Batch 605, Loss: 2.2407429218292236\n",
      "Epoch 5, Batch 610, Loss: 2.259230136871338\n",
      "Epoch 5, Batch 615, Loss: 2.454585313796997\n",
      "Epoch 5, Batch 620, Loss: 2.474597215652466\n",
      "Epoch 5, Batch 625, Loss: 2.368311882019043\n",
      "Epoch 5, Batch 630, Loss: 2.261807918548584\n",
      "Epoch 5, Batch 635, Loss: 2.5328474044799805\n",
      "Epoch 5, Batch 640, Loss: 2.1693801879882812\n",
      "Epoch 5, Batch 645, Loss: 2.374891519546509\n",
      "Epoch 5, Batch 650, Loss: 2.417494773864746\n",
      "Epoch 5, Batch 655, Loss: 2.4242594242095947\n",
      "Epoch 6, Batch 0, Loss: 2.347787618637085\n",
      "Epoch 6, Batch 5, Loss: 2.3423521518707275\n",
      "Epoch 6, Batch 10, Loss: 2.3215761184692383\n",
      "Epoch 6, Batch 15, Loss: 2.4111101627349854\n",
      "Epoch 6, Batch 20, Loss: 2.4178764820098877\n",
      "Epoch 6, Batch 25, Loss: 2.328873634338379\n",
      "Epoch 6, Batch 30, Loss: 2.1980364322662354\n",
      "Epoch 6, Batch 35, Loss: 2.4841530323028564\n",
      "Epoch 6, Batch 40, Loss: 2.2763068675994873\n",
      "Epoch 6, Batch 45, Loss: 2.4480199813842773\n",
      "Epoch 6, Batch 50, Loss: 2.237671136856079\n",
      "Epoch 6, Batch 55, Loss: 2.288942337036133\n",
      "Epoch 6, Batch 60, Loss: 2.2885806560516357\n",
      "Epoch 6, Batch 65, Loss: 2.3582875728607178\n",
      "Epoch 6, Batch 70, Loss: 2.454568862915039\n",
      "Epoch 6, Batch 75, Loss: 2.2707619667053223\n",
      "Epoch 6, Batch 80, Loss: 2.2562475204467773\n",
      "Epoch 6, Batch 85, Loss: 2.3087241649627686\n",
      "Epoch 6, Batch 90, Loss: 2.2930843830108643\n",
      "Epoch 6, Batch 95, Loss: 2.249324083328247\n",
      "Epoch 6, Batch 100, Loss: 2.378445863723755\n",
      "Epoch 6, Batch 105, Loss: 2.3040060997009277\n",
      "Epoch 6, Batch 110, Loss: 2.323500394821167\n",
      "Epoch 6, Batch 115, Loss: 2.3307383060455322\n",
      "Epoch 6, Batch 120, Loss: 2.569941997528076\n",
      "Epoch 6, Batch 125, Loss: 2.4087817668914795\n",
      "Epoch 6, Batch 130, Loss: 2.219698190689087\n",
      "Epoch 6, Batch 135, Loss: 2.337024211883545\n",
      "Epoch 6, Batch 140, Loss: 2.3246848583221436\n",
      "Epoch 6, Batch 145, Loss: 2.2597193717956543\n",
      "Epoch 6, Batch 150, Loss: 2.436898708343506\n",
      "Epoch 6, Batch 155, Loss: 2.500324010848999\n",
      "Epoch 6, Batch 160, Loss: 2.3031656742095947\n",
      "Epoch 6, Batch 165, Loss: 2.4662082195281982\n",
      "Epoch 6, Batch 170, Loss: 2.4452414512634277\n",
      "Epoch 6, Batch 175, Loss: 2.4688684940338135\n",
      "Epoch 6, Batch 180, Loss: 2.483322858810425\n",
      "Epoch 6, Batch 185, Loss: 2.4823944568634033\n",
      "Epoch 6, Batch 190, Loss: 2.363842010498047\n",
      "Epoch 6, Batch 195, Loss: 2.3773868083953857\n",
      "Epoch 6, Batch 200, Loss: 2.7005739212036133\n",
      "Epoch 6, Batch 205, Loss: 2.2751500606536865\n",
      "Epoch 6, Batch 210, Loss: 2.3965892791748047\n",
      "Epoch 6, Batch 215, Loss: 2.494290828704834\n",
      "Epoch 6, Batch 220, Loss: 2.2634263038635254\n",
      "Epoch 6, Batch 225, Loss: 2.487565040588379\n",
      "Epoch 6, Batch 230, Loss: 2.427032709121704\n",
      "Epoch 6, Batch 235, Loss: 2.2892823219299316\n",
      "Epoch 6, Batch 240, Loss: 2.369312047958374\n",
      "Epoch 6, Batch 245, Loss: 2.2380731105804443\n",
      "Epoch 6, Batch 250, Loss: 2.335254430770874\n",
      "Epoch 6, Batch 255, Loss: 2.089216709136963\n",
      "Epoch 6, Batch 260, Loss: 2.2959625720977783\n",
      "Epoch 6, Batch 265, Loss: 2.317699909210205\n",
      "Epoch 6, Batch 270, Loss: 2.393064022064209\n",
      "Epoch 6, Batch 275, Loss: 2.4708375930786133\n",
      "Epoch 6, Batch 280, Loss: 2.2329182624816895\n",
      "Epoch 6, Batch 285, Loss: 2.600127935409546\n",
      "Epoch 6, Batch 290, Loss: 2.157998561859131\n",
      "Epoch 6, Batch 295, Loss: 2.584963083267212\n",
      "Epoch 6, Batch 300, Loss: 2.379276990890503\n",
      "Epoch 6, Batch 305, Loss: 2.2933084964752197\n",
      "Epoch 6, Batch 310, Loss: 2.3581576347351074\n",
      "Epoch 6, Batch 315, Loss: 2.34394907951355\n",
      "Epoch 6, Batch 320, Loss: 2.2394258975982666\n",
      "Epoch 6, Batch 325, Loss: 2.281329870223999\n",
      "Epoch 6, Batch 330, Loss: 2.517972707748413\n",
      "Epoch 6, Batch 335, Loss: 2.335209846496582\n",
      "Epoch 6, Batch 340, Loss: 2.123051881790161\n",
      "Epoch 6, Batch 345, Loss: 2.2792601585388184\n",
      "Epoch 6, Batch 350, Loss: 2.312025308609009\n",
      "Epoch 6, Batch 355, Loss: 2.4616916179656982\n",
      "Epoch 6, Batch 360, Loss: 2.230395555496216\n",
      "Epoch 6, Batch 365, Loss: 2.195657730102539\n",
      "Epoch 6, Batch 370, Loss: 2.477818012237549\n",
      "Epoch 6, Batch 375, Loss: 2.3806047439575195\n",
      "Epoch 6, Batch 380, Loss: 2.371385097503662\n",
      "Epoch 6, Batch 385, Loss: 2.475813865661621\n",
      "Epoch 6, Batch 390, Loss: 2.455796718597412\n",
      "Epoch 6, Batch 395, Loss: 2.305760145187378\n",
      "Epoch 6, Batch 400, Loss: 2.287644863128662\n",
      "Epoch 6, Batch 405, Loss: 2.3503572940826416\n",
      "Epoch 6, Batch 410, Loss: 2.3582773208618164\n",
      "Epoch 6, Batch 415, Loss: 2.2764689922332764\n",
      "Epoch 6, Batch 420, Loss: 2.3380284309387207\n",
      "Epoch 6, Batch 425, Loss: 2.4515490531921387\n",
      "Epoch 6, Batch 430, Loss: 2.1765198707580566\n",
      "Epoch 6, Batch 435, Loss: 2.732588529586792\n",
      "Epoch 6, Batch 440, Loss: 2.421889543533325\n",
      "Epoch 6, Batch 445, Loss: 2.447500705718994\n",
      "Epoch 6, Batch 450, Loss: 2.1630771160125732\n",
      "Epoch 6, Batch 455, Loss: 2.3530170917510986\n",
      "Epoch 6, Batch 460, Loss: 2.261998176574707\n",
      "Epoch 6, Batch 465, Loss: 2.3158669471740723\n",
      "Epoch 6, Batch 470, Loss: 2.2623958587646484\n",
      "Epoch 6, Batch 475, Loss: 2.191138744354248\n",
      "Epoch 6, Batch 480, Loss: 2.142036199569702\n",
      "Epoch 6, Batch 485, Loss: 2.5957727432250977\n",
      "Epoch 6, Batch 490, Loss: 2.3505494594573975\n",
      "Epoch 6, Batch 495, Loss: 2.4470152854919434\n",
      "Epoch 6, Batch 500, Loss: 2.5206973552703857\n",
      "Epoch 6, Batch 505, Loss: 2.209763765335083\n",
      "Epoch 6, Batch 510, Loss: 2.4622488021850586\n",
      "Epoch 6, Batch 515, Loss: 2.450202703475952\n",
      "Epoch 6, Batch 520, Loss: 2.3308653831481934\n",
      "Epoch 6, Batch 525, Loss: 2.346672296524048\n",
      "Epoch 6, Batch 530, Loss: 2.1404805183410645\n",
      "Epoch 6, Batch 535, Loss: 2.3484251499176025\n",
      "Epoch 6, Batch 540, Loss: 2.2944717407226562\n",
      "Epoch 6, Batch 545, Loss: 2.177345037460327\n",
      "Epoch 6, Batch 550, Loss: 2.3468761444091797\n",
      "Epoch 6, Batch 555, Loss: 2.386280059814453\n",
      "Epoch 6, Batch 560, Loss: 2.421497344970703\n",
      "Epoch 6, Batch 565, Loss: 2.4624979496002197\n",
      "Epoch 6, Batch 570, Loss: 2.4717676639556885\n",
      "Epoch 6, Batch 575, Loss: 2.420013666152954\n",
      "Epoch 6, Batch 580, Loss: 2.4247617721557617\n",
      "Epoch 6, Batch 585, Loss: 2.327091932296753\n",
      "Epoch 6, Batch 590, Loss: 2.2457356452941895\n",
      "Epoch 6, Batch 595, Loss: 2.265803098678589\n",
      "Epoch 6, Batch 600, Loss: 2.331599235534668\n",
      "Epoch 6, Batch 605, Loss: 2.291029930114746\n",
      "Epoch 6, Batch 610, Loss: 2.333005666732788\n",
      "Epoch 6, Batch 615, Loss: 2.1832199096679688\n",
      "Epoch 6, Batch 620, Loss: 2.3318893909454346\n",
      "Epoch 6, Batch 625, Loss: 2.3778536319732666\n",
      "Epoch 6, Batch 630, Loss: 2.2741858959198\n",
      "Epoch 6, Batch 635, Loss: 2.4384772777557373\n",
      "Epoch 6, Batch 640, Loss: 2.3403847217559814\n",
      "Epoch 6, Batch 645, Loss: 2.2345364093780518\n",
      "Epoch 6, Batch 650, Loss: 2.0908937454223633\n",
      "Epoch 6, Batch 655, Loss: 2.440915107727051\n",
      "Epoch 7, Batch 0, Loss: 2.3292741775512695\n",
      "Epoch 7, Batch 5, Loss: 2.337054491043091\n",
      "Epoch 7, Batch 10, Loss: 2.149984121322632\n",
      "Epoch 7, Batch 15, Loss: 2.3893682956695557\n",
      "Epoch 7, Batch 20, Loss: 2.3606908321380615\n",
      "Epoch 7, Batch 25, Loss: 2.297741413116455\n",
      "Epoch 7, Batch 30, Loss: 2.3470616340637207\n",
      "Epoch 7, Batch 35, Loss: 2.41373348236084\n",
      "Epoch 7, Batch 40, Loss: 2.241868257522583\n",
      "Epoch 7, Batch 45, Loss: 2.2869489192962646\n",
      "Epoch 7, Batch 50, Loss: 2.2250444889068604\n",
      "Epoch 7, Batch 55, Loss: 2.46151065826416\n",
      "Epoch 7, Batch 60, Loss: 2.3633873462677\n",
      "Epoch 7, Batch 65, Loss: 2.330251693725586\n",
      "Epoch 7, Batch 70, Loss: 2.3381357192993164\n",
      "Epoch 7, Batch 75, Loss: 2.249565839767456\n",
      "Epoch 7, Batch 80, Loss: 2.109626531600952\n",
      "Epoch 7, Batch 85, Loss: 2.4556772708892822\n",
      "Epoch 7, Batch 90, Loss: 2.3960769176483154\n",
      "Epoch 7, Batch 95, Loss: 2.252110481262207\n",
      "Epoch 7, Batch 100, Loss: 2.3304314613342285\n",
      "Epoch 7, Batch 105, Loss: 2.433774948120117\n",
      "Epoch 7, Batch 110, Loss: 2.3915228843688965\n",
      "Epoch 7, Batch 115, Loss: 2.2752625942230225\n",
      "Epoch 7, Batch 120, Loss: 2.268096685409546\n",
      "Epoch 7, Batch 125, Loss: 2.280867576599121\n",
      "Epoch 7, Batch 130, Loss: 2.2462828159332275\n",
      "Epoch 7, Batch 135, Loss: 2.2197813987731934\n",
      "Epoch 7, Batch 140, Loss: 2.300424575805664\n",
      "Epoch 7, Batch 145, Loss: 2.354867935180664\n",
      "Epoch 7, Batch 150, Loss: 2.3540198802948\n",
      "Epoch 7, Batch 155, Loss: 2.4877207279205322\n",
      "Epoch 7, Batch 160, Loss: 2.462207317352295\n",
      "Epoch 7, Batch 165, Loss: 2.293532371520996\n",
      "Epoch 7, Batch 170, Loss: 2.3052000999450684\n",
      "Epoch 7, Batch 175, Loss: 2.118644952774048\n",
      "Epoch 7, Batch 180, Loss: 2.2668864727020264\n",
      "Epoch 7, Batch 185, Loss: 2.31552791595459\n",
      "Epoch 7, Batch 190, Loss: 2.134044647216797\n",
      "Epoch 7, Batch 195, Loss: 2.554753303527832\n",
      "Epoch 7, Batch 200, Loss: 2.2715795040130615\n",
      "Epoch 7, Batch 205, Loss: 2.3459243774414062\n",
      "Epoch 7, Batch 210, Loss: 2.246551275253296\n",
      "Epoch 7, Batch 215, Loss: 2.4775049686431885\n",
      "Epoch 7, Batch 220, Loss: 2.3003060817718506\n",
      "Epoch 7, Batch 225, Loss: 2.2971253395080566\n",
      "Epoch 7, Batch 230, Loss: 2.4973058700561523\n",
      "Epoch 7, Batch 235, Loss: 2.282261610031128\n",
      "Epoch 7, Batch 240, Loss: 2.3708651065826416\n",
      "Epoch 7, Batch 245, Loss: 2.36480450630188\n",
      "Epoch 7, Batch 250, Loss: 2.4347901344299316\n",
      "Epoch 7, Batch 255, Loss: 2.5498104095458984\n",
      "Epoch 7, Batch 260, Loss: 2.4133565425872803\n",
      "Epoch 7, Batch 265, Loss: 2.359950542449951\n",
      "Epoch 7, Batch 270, Loss: 2.4850707054138184\n",
      "Epoch 7, Batch 275, Loss: 2.2011325359344482\n",
      "Epoch 7, Batch 280, Loss: 2.4609785079956055\n",
      "Epoch 7, Batch 285, Loss: 2.1543266773223877\n",
      "Epoch 7, Batch 290, Loss: 2.2330689430236816\n",
      "Epoch 7, Batch 295, Loss: 2.3456151485443115\n",
      "Epoch 7, Batch 300, Loss: 2.4704298973083496\n",
      "Epoch 7, Batch 305, Loss: 2.461874485015869\n",
      "Epoch 7, Batch 310, Loss: 2.4360527992248535\n",
      "Epoch 7, Batch 315, Loss: 2.585395574569702\n",
      "Epoch 7, Batch 320, Loss: 2.306809663772583\n",
      "Epoch 7, Batch 325, Loss: 2.388362169265747\n",
      "Epoch 7, Batch 330, Loss: 2.389735698699951\n",
      "Epoch 7, Batch 335, Loss: 2.384204864501953\n",
      "Epoch 7, Batch 340, Loss: 2.4128665924072266\n",
      "Epoch 7, Batch 345, Loss: 2.309295892715454\n",
      "Epoch 7, Batch 350, Loss: 2.1210806369781494\n",
      "Epoch 7, Batch 355, Loss: 2.3483328819274902\n",
      "Epoch 7, Batch 360, Loss: 2.0607666969299316\n",
      "Epoch 7, Batch 365, Loss: 2.2735652923583984\n",
      "Epoch 7, Batch 370, Loss: 2.3057994842529297\n",
      "Epoch 7, Batch 375, Loss: 2.295496940612793\n",
      "Epoch 7, Batch 380, Loss: 2.2607929706573486\n",
      "Epoch 7, Batch 385, Loss: 2.3816511631011963\n",
      "Epoch 7, Batch 390, Loss: 2.5043065547943115\n",
      "Epoch 7, Batch 395, Loss: 2.2372844219207764\n",
      "Epoch 7, Batch 400, Loss: 2.3433077335357666\n",
      "Epoch 7, Batch 405, Loss: 2.4466211795806885\n",
      "Epoch 7, Batch 410, Loss: 2.2701711654663086\n",
      "Epoch 7, Batch 415, Loss: 2.3246636390686035\n",
      "Epoch 7, Batch 420, Loss: 2.1875498294830322\n",
      "Epoch 7, Batch 425, Loss: 2.339818239212036\n",
      "Epoch 7, Batch 430, Loss: 2.1788742542266846\n",
      "Epoch 7, Batch 435, Loss: 2.338346242904663\n",
      "Epoch 7, Batch 440, Loss: 2.4709818363189697\n",
      "Epoch 7, Batch 445, Loss: 2.482325553894043\n",
      "Epoch 7, Batch 450, Loss: 2.263723373413086\n",
      "Epoch 7, Batch 455, Loss: 2.4481260776519775\n",
      "Epoch 7, Batch 460, Loss: 2.5283048152923584\n",
      "Epoch 7, Batch 465, Loss: 2.4512434005737305\n",
      "Epoch 7, Batch 470, Loss: 2.3002500534057617\n",
      "Epoch 7, Batch 475, Loss: 2.427839994430542\n",
      "Epoch 7, Batch 480, Loss: 2.2470223903656006\n",
      "Epoch 7, Batch 485, Loss: 2.3221802711486816\n",
      "Epoch 7, Batch 490, Loss: 2.3685078620910645\n",
      "Epoch 7, Batch 495, Loss: 2.3018224239349365\n",
      "Epoch 7, Batch 500, Loss: 2.405780792236328\n",
      "Epoch 7, Batch 505, Loss: 2.0527548789978027\n",
      "Epoch 7, Batch 510, Loss: 2.377756357192993\n",
      "Epoch 7, Batch 515, Loss: 2.3823835849761963\n",
      "Epoch 7, Batch 520, Loss: 2.3637232780456543\n",
      "Epoch 7, Batch 525, Loss: 2.3452353477478027\n",
      "Epoch 7, Batch 530, Loss: 2.203533172607422\n",
      "Epoch 7, Batch 535, Loss: 2.4018044471740723\n",
      "Epoch 7, Batch 540, Loss: 2.2560935020446777\n",
      "Epoch 7, Batch 545, Loss: 2.099503993988037\n",
      "Epoch 7, Batch 550, Loss: 2.3931357860565186\n",
      "Epoch 7, Batch 555, Loss: 2.307110071182251\n",
      "Epoch 7, Batch 560, Loss: 2.338486909866333\n",
      "Epoch 7, Batch 565, Loss: 2.201788902282715\n",
      "Epoch 7, Batch 570, Loss: 2.5767617225646973\n",
      "Epoch 7, Batch 575, Loss: 2.5488829612731934\n",
      "Epoch 7, Batch 580, Loss: 2.277717113494873\n",
      "Epoch 7, Batch 585, Loss: 2.2938075065612793\n",
      "Epoch 7, Batch 590, Loss: 2.4142367839813232\n",
      "Epoch 7, Batch 595, Loss: 2.356536388397217\n",
      "Epoch 7, Batch 600, Loss: 2.517650604248047\n",
      "Epoch 7, Batch 605, Loss: 2.316321849822998\n",
      "Epoch 7, Batch 610, Loss: 2.4504005908966064\n",
      "Epoch 7, Batch 615, Loss: 2.1328160762786865\n",
      "Epoch 7, Batch 620, Loss: 2.2953927516937256\n",
      "Epoch 7, Batch 625, Loss: 2.176276206970215\n",
      "Epoch 7, Batch 630, Loss: 2.2179598808288574\n",
      "Epoch 7, Batch 635, Loss: 2.1414129734039307\n",
      "Epoch 7, Batch 640, Loss: 2.3027706146240234\n",
      "Epoch 7, Batch 645, Loss: 2.338470697402954\n",
      "Epoch 7, Batch 650, Loss: 2.4205329418182373\n",
      "Epoch 7, Batch 655, Loss: 2.383389949798584\n",
      "Epoch 8, Batch 0, Loss: 2.333871841430664\n",
      "Epoch 8, Batch 5, Loss: 2.304941415786743\n",
      "Epoch 8, Batch 10, Loss: 2.308253288269043\n",
      "Epoch 8, Batch 15, Loss: 2.209726333618164\n",
      "Epoch 8, Batch 20, Loss: 2.3787574768066406\n",
      "Epoch 8, Batch 25, Loss: 2.543316602706909\n",
      "Epoch 8, Batch 30, Loss: 2.280123233795166\n",
      "Epoch 8, Batch 35, Loss: 2.447272300720215\n",
      "Epoch 8, Batch 40, Loss: 2.4430551528930664\n",
      "Epoch 8, Batch 45, Loss: 2.256711721420288\n",
      "Epoch 8, Batch 50, Loss: 2.38436222076416\n",
      "Epoch 8, Batch 55, Loss: 2.4395928382873535\n",
      "Epoch 8, Batch 60, Loss: 2.3299968242645264\n",
      "Epoch 8, Batch 65, Loss: 2.3655476570129395\n",
      "Epoch 8, Batch 70, Loss: 2.48907470703125\n",
      "Epoch 8, Batch 75, Loss: 2.4832334518432617\n",
      "Epoch 8, Batch 80, Loss: 2.491173267364502\n",
      "Epoch 8, Batch 85, Loss: 2.135239839553833\n",
      "Epoch 8, Batch 90, Loss: 2.406406879425049\n",
      "Epoch 8, Batch 95, Loss: 2.094233989715576\n",
      "Epoch 8, Batch 100, Loss: 2.4196927547454834\n",
      "Epoch 8, Batch 105, Loss: 2.3672211170196533\n",
      "Epoch 8, Batch 110, Loss: 2.187742233276367\n",
      "Epoch 8, Batch 115, Loss: 2.417614698410034\n",
      "Epoch 8, Batch 120, Loss: 2.3142354488372803\n",
      "Epoch 8, Batch 125, Loss: 2.2924375534057617\n",
      "Epoch 8, Batch 130, Loss: 2.5074222087860107\n",
      "Epoch 8, Batch 135, Loss: 2.2537944316864014\n",
      "Epoch 8, Batch 140, Loss: 2.308758020401001\n",
      "Epoch 8, Batch 145, Loss: 2.1076276302337646\n",
      "Epoch 8, Batch 150, Loss: 2.5273237228393555\n",
      "Epoch 8, Batch 155, Loss: 2.527913808822632\n",
      "Epoch 8, Batch 160, Loss: 2.653214931488037\n",
      "Epoch 8, Batch 165, Loss: 2.4929752349853516\n",
      "Epoch 8, Batch 170, Loss: 2.4413368701934814\n",
      "Epoch 8, Batch 175, Loss: 2.3946614265441895\n",
      "Epoch 8, Batch 180, Loss: 2.3825156688690186\n",
      "Epoch 8, Batch 185, Loss: 2.419297695159912\n",
      "Epoch 8, Batch 190, Loss: 2.4438982009887695\n",
      "Epoch 8, Batch 195, Loss: 2.3667688369750977\n",
      "Epoch 8, Batch 200, Loss: 2.2138147354125977\n",
      "Epoch 8, Batch 205, Loss: 2.3539161682128906\n",
      "Epoch 8, Batch 210, Loss: 2.301849126815796\n",
      "Epoch 8, Batch 215, Loss: 2.291916608810425\n",
      "Epoch 8, Batch 220, Loss: 2.318885326385498\n",
      "Epoch 8, Batch 225, Loss: 2.3417372703552246\n",
      "Epoch 8, Batch 230, Loss: 2.077387571334839\n",
      "Epoch 8, Batch 235, Loss: 2.312654972076416\n",
      "Epoch 8, Batch 240, Loss: 2.407961845397949\n",
      "Epoch 8, Batch 245, Loss: 2.2871556282043457\n",
      "Epoch 8, Batch 250, Loss: 2.5465176105499268\n",
      "Epoch 8, Batch 255, Loss: 2.3182106018066406\n",
      "Epoch 8, Batch 260, Loss: 2.397865056991577\n",
      "Epoch 8, Batch 265, Loss: 2.4203643798828125\n",
      "Epoch 8, Batch 270, Loss: 2.307717800140381\n",
      "Epoch 8, Batch 275, Loss: 2.328143835067749\n",
      "Epoch 8, Batch 280, Loss: 2.4330968856811523\n",
      "Epoch 8, Batch 285, Loss: 2.337466239929199\n",
      "Epoch 8, Batch 290, Loss: 2.299386978149414\n",
      "Epoch 8, Batch 295, Loss: 2.217683792114258\n",
      "Epoch 8, Batch 300, Loss: 2.219944953918457\n",
      "Epoch 8, Batch 305, Loss: 2.391519784927368\n",
      "Epoch 8, Batch 310, Loss: 2.2466657161712646\n",
      "Epoch 8, Batch 315, Loss: 2.3840713500976562\n",
      "Epoch 8, Batch 320, Loss: 2.4463109970092773\n",
      "Epoch 8, Batch 325, Loss: 2.35416316986084\n",
      "Epoch 8, Batch 330, Loss: 2.38138484954834\n",
      "Epoch 8, Batch 335, Loss: 2.2826082706451416\n",
      "Epoch 8, Batch 340, Loss: 2.275406837463379\n",
      "Epoch 8, Batch 345, Loss: 2.355741500854492\n",
      "Epoch 8, Batch 350, Loss: 2.44831919670105\n",
      "Epoch 8, Batch 355, Loss: 2.2338151931762695\n",
      "Epoch 8, Batch 360, Loss: 2.169536828994751\n",
      "Epoch 8, Batch 365, Loss: 2.4677200317382812\n",
      "Epoch 8, Batch 370, Loss: 2.4618804454803467\n",
      "Epoch 8, Batch 375, Loss: 2.3859448432922363\n",
      "Epoch 8, Batch 380, Loss: 2.221564531326294\n",
      "Epoch 8, Batch 385, Loss: 2.220656633377075\n",
      "Epoch 8, Batch 390, Loss: 2.334045886993408\n",
      "Epoch 8, Batch 395, Loss: 2.400411605834961\n",
      "Epoch 8, Batch 400, Loss: 2.280754566192627\n",
      "Epoch 8, Batch 405, Loss: 2.2937092781066895\n",
      "Epoch 8, Batch 410, Loss: 2.2599692344665527\n",
      "Epoch 8, Batch 415, Loss: 2.455456256866455\n",
      "Epoch 8, Batch 420, Loss: 2.460222005844116\n",
      "Epoch 8, Batch 425, Loss: 2.236830711364746\n",
      "Epoch 8, Batch 430, Loss: 2.24617862701416\n",
      "Epoch 8, Batch 435, Loss: 2.283628225326538\n",
      "Epoch 8, Batch 440, Loss: 2.5431172847747803\n",
      "Epoch 8, Batch 445, Loss: 2.301302909851074\n",
      "Epoch 8, Batch 450, Loss: 2.5184152126312256\n",
      "Epoch 8, Batch 455, Loss: 2.3441085815429688\n",
      "Epoch 8, Batch 460, Loss: 2.4619386196136475\n",
      "Epoch 8, Batch 465, Loss: 2.389934778213501\n",
      "Epoch 8, Batch 470, Loss: 2.379034996032715\n",
      "Epoch 8, Batch 475, Loss: 2.2771716117858887\n",
      "Epoch 8, Batch 480, Loss: 2.2916224002838135\n",
      "Epoch 8, Batch 485, Loss: 2.4474666118621826\n",
      "Epoch 8, Batch 490, Loss: 2.4364256858825684\n",
      "Epoch 8, Batch 495, Loss: 2.163930654525757\n",
      "Epoch 8, Batch 500, Loss: 2.2511870861053467\n",
      "Epoch 8, Batch 505, Loss: 2.4560129642486572\n",
      "Epoch 8, Batch 510, Loss: 2.2081544399261475\n",
      "Epoch 8, Batch 515, Loss: 2.536316394805908\n",
      "Epoch 8, Batch 520, Loss: 2.2138915061950684\n",
      "Epoch 8, Batch 525, Loss: 2.3475067615509033\n",
      "Epoch 8, Batch 530, Loss: 2.2642455101013184\n",
      "Epoch 8, Batch 535, Loss: 2.164426326751709\n",
      "Epoch 8, Batch 540, Loss: 2.197261095046997\n",
      "Epoch 8, Batch 545, Loss: 2.3790981769561768\n",
      "Epoch 8, Batch 550, Loss: 2.3090932369232178\n",
      "Epoch 8, Batch 555, Loss: 2.3250927925109863\n",
      "Epoch 8, Batch 560, Loss: 2.1565756797790527\n",
      "Epoch 8, Batch 565, Loss: 2.3403525352478027\n",
      "Epoch 8, Batch 570, Loss: 2.4453954696655273\n",
      "Epoch 8, Batch 575, Loss: 2.449402332305908\n",
      "Epoch 8, Batch 580, Loss: 2.429699420928955\n",
      "Epoch 8, Batch 585, Loss: 2.316114664077759\n",
      "Epoch 8, Batch 590, Loss: 2.3292272090911865\n",
      "Epoch 8, Batch 595, Loss: 2.3303771018981934\n",
      "Epoch 8, Batch 600, Loss: 2.498255729675293\n",
      "Epoch 8, Batch 605, Loss: 2.1472649574279785\n",
      "Epoch 8, Batch 610, Loss: 2.324448347091675\n",
      "Epoch 8, Batch 615, Loss: 2.343264102935791\n",
      "Epoch 8, Batch 620, Loss: 2.4564177989959717\n",
      "Epoch 8, Batch 625, Loss: 2.22174334526062\n",
      "Epoch 8, Batch 630, Loss: 2.4737868309020996\n",
      "Epoch 8, Batch 635, Loss: 2.394871473312378\n",
      "Epoch 8, Batch 640, Loss: 2.406433343887329\n",
      "Epoch 8, Batch 645, Loss: 2.2928566932678223\n",
      "Epoch 8, Batch 650, Loss: 2.370898723602295\n",
      "Epoch 8, Batch 655, Loss: 2.3490419387817383\n",
      "Epoch 9, Batch 0, Loss: 2.3308892250061035\n",
      "Epoch 9, Batch 5, Loss: 2.312957286834717\n",
      "Epoch 9, Batch 10, Loss: 2.4583780765533447\n",
      "Epoch 9, Batch 15, Loss: 2.372504472732544\n",
      "Epoch 9, Batch 20, Loss: 2.2705817222595215\n",
      "Epoch 9, Batch 25, Loss: 2.4591922760009766\n",
      "Epoch 9, Batch 30, Loss: 2.391895055770874\n",
      "Epoch 9, Batch 35, Loss: 2.14042067527771\n",
      "Epoch 9, Batch 40, Loss: 2.288902759552002\n",
      "Epoch 9, Batch 45, Loss: 2.199885606765747\n",
      "Epoch 9, Batch 50, Loss: 2.1660468578338623\n",
      "Epoch 9, Batch 55, Loss: 2.4056084156036377\n",
      "Epoch 9, Batch 60, Loss: 2.310245990753174\n",
      "Epoch 9, Batch 65, Loss: 2.3123579025268555\n",
      "Epoch 9, Batch 70, Loss: 2.466129779815674\n",
      "Epoch 9, Batch 75, Loss: 2.4676663875579834\n",
      "Epoch 9, Batch 80, Loss: 2.5185210704803467\n",
      "Epoch 9, Batch 85, Loss: 2.3568410873413086\n",
      "Epoch 9, Batch 90, Loss: 2.256882429122925\n",
      "Epoch 9, Batch 95, Loss: 2.3000853061676025\n",
      "Epoch 9, Batch 100, Loss: 2.4201321601867676\n",
      "Epoch 9, Batch 105, Loss: 2.512714147567749\n",
      "Epoch 9, Batch 110, Loss: 2.384864091873169\n",
      "Epoch 9, Batch 115, Loss: 2.2138402462005615\n",
      "Epoch 9, Batch 120, Loss: 2.4165258407592773\n",
      "Epoch 9, Batch 125, Loss: 2.3584961891174316\n",
      "Epoch 9, Batch 130, Loss: 2.4796030521392822\n",
      "Epoch 9, Batch 135, Loss: 2.2060790061950684\n",
      "Epoch 9, Batch 140, Loss: 2.409381151199341\n",
      "Epoch 9, Batch 145, Loss: 2.5259392261505127\n",
      "Epoch 9, Batch 150, Loss: 2.4040403366088867\n",
      "Epoch 9, Batch 155, Loss: 2.38163685798645\n",
      "Epoch 9, Batch 160, Loss: 2.3955931663513184\n",
      "Epoch 9, Batch 165, Loss: 2.1643166542053223\n",
      "Epoch 9, Batch 170, Loss: 2.2899086475372314\n",
      "Epoch 9, Batch 175, Loss: 2.446475028991699\n",
      "Epoch 9, Batch 180, Loss: 2.4213123321533203\n",
      "Epoch 9, Batch 185, Loss: 2.1359968185424805\n",
      "Epoch 9, Batch 190, Loss: 2.25288462638855\n",
      "Epoch 9, Batch 195, Loss: 2.3878586292266846\n",
      "Epoch 9, Batch 200, Loss: 2.5314598083496094\n",
      "Epoch 9, Batch 205, Loss: 2.468350410461426\n",
      "Epoch 9, Batch 210, Loss: 2.3176064491271973\n",
      "Epoch 9, Batch 215, Loss: 2.4439079761505127\n",
      "Epoch 9, Batch 220, Loss: 2.3341853618621826\n",
      "Epoch 9, Batch 225, Loss: 2.3770925998687744\n",
      "Epoch 9, Batch 230, Loss: 2.28239369392395\n",
      "Epoch 9, Batch 235, Loss: 2.4289536476135254\n",
      "Epoch 9, Batch 240, Loss: 2.19623064994812\n",
      "Epoch 9, Batch 245, Loss: 2.1455512046813965\n",
      "Epoch 9, Batch 250, Loss: 2.3051202297210693\n",
      "Epoch 9, Batch 255, Loss: 2.3092143535614014\n",
      "Epoch 9, Batch 260, Loss: 2.501793622970581\n",
      "Epoch 9, Batch 265, Loss: 2.3307056427001953\n",
      "Epoch 9, Batch 270, Loss: 2.1801347732543945\n",
      "Epoch 9, Batch 275, Loss: 2.25921630859375\n",
      "Epoch 9, Batch 280, Loss: 2.429560661315918\n",
      "Epoch 9, Batch 285, Loss: 2.4123475551605225\n",
      "Epoch 9, Batch 290, Loss: 2.5051796436309814\n",
      "Epoch 9, Batch 295, Loss: 2.339662790298462\n",
      "Epoch 9, Batch 300, Loss: 2.45352840423584\n",
      "Epoch 9, Batch 305, Loss: 2.359208583831787\n",
      "Epoch 9, Batch 310, Loss: 2.2147645950317383\n",
      "Epoch 9, Batch 315, Loss: 2.256669521331787\n",
      "Epoch 9, Batch 320, Loss: 2.397545099258423\n",
      "Epoch 9, Batch 325, Loss: 2.4102303981781006\n",
      "Epoch 9, Batch 330, Loss: 2.4394114017486572\n",
      "Epoch 9, Batch 335, Loss: 2.275676965713501\n",
      "Epoch 9, Batch 340, Loss: 2.3658089637756348\n",
      "Epoch 9, Batch 345, Loss: 2.3315799236297607\n",
      "Epoch 9, Batch 350, Loss: 2.3366761207580566\n",
      "Epoch 9, Batch 355, Loss: 2.4168622493743896\n",
      "Epoch 9, Batch 360, Loss: 2.421048164367676\n",
      "Epoch 9, Batch 365, Loss: 2.3317556381225586\n",
      "Epoch 9, Batch 370, Loss: 2.2811875343322754\n",
      "Epoch 9, Batch 375, Loss: 2.214472770690918\n",
      "Epoch 9, Batch 380, Loss: 2.375624656677246\n",
      "Epoch 9, Batch 385, Loss: 2.3299341201782227\n",
      "Epoch 9, Batch 390, Loss: 2.506807565689087\n",
      "Epoch 9, Batch 395, Loss: 2.198136329650879\n",
      "Epoch 9, Batch 400, Loss: 2.207979679107666\n",
      "Epoch 9, Batch 405, Loss: 2.2657201290130615\n",
      "Epoch 9, Batch 410, Loss: 2.0777475833892822\n",
      "Epoch 9, Batch 415, Loss: 2.207836151123047\n",
      "Epoch 9, Batch 420, Loss: 2.562030553817749\n",
      "Epoch 9, Batch 425, Loss: 2.3262598514556885\n",
      "Epoch 9, Batch 430, Loss: 2.5201358795166016\n",
      "Epoch 9, Batch 435, Loss: 2.410797357559204\n",
      "Epoch 9, Batch 440, Loss: 2.374648094177246\n",
      "Epoch 9, Batch 445, Loss: 2.343104124069214\n",
      "Epoch 9, Batch 450, Loss: 2.373720407485962\n",
      "Epoch 9, Batch 455, Loss: 2.0368616580963135\n",
      "Epoch 9, Batch 460, Loss: 2.2101433277130127\n",
      "Epoch 9, Batch 465, Loss: 2.5421459674835205\n",
      "Epoch 9, Batch 470, Loss: 2.3515331745147705\n",
      "Epoch 9, Batch 475, Loss: 2.48820424079895\n",
      "Epoch 9, Batch 480, Loss: 2.481550693511963\n",
      "Epoch 9, Batch 485, Loss: 2.2711689472198486\n",
      "Epoch 9, Batch 490, Loss: 2.187354326248169\n",
      "Epoch 9, Batch 495, Loss: 2.3883275985717773\n",
      "Epoch 9, Batch 500, Loss: 2.3744592666625977\n",
      "Epoch 9, Batch 505, Loss: 2.245138645172119\n",
      "Epoch 9, Batch 510, Loss: 2.1671082973480225\n",
      "Epoch 9, Batch 515, Loss: 2.416969060897827\n",
      "Epoch 9, Batch 520, Loss: 2.304609775543213\n",
      "Epoch 9, Batch 525, Loss: 2.2740848064422607\n",
      "Epoch 9, Batch 530, Loss: 2.078118085861206\n",
      "Epoch 9, Batch 535, Loss: 2.4054670333862305\n",
      "Epoch 9, Batch 540, Loss: 2.618598699569702\n",
      "Epoch 9, Batch 545, Loss: 2.354691982269287\n",
      "Epoch 9, Batch 550, Loss: 2.398282766342163\n",
      "Epoch 9, Batch 555, Loss: 2.495791435241699\n",
      "Epoch 9, Batch 560, Loss: 2.500164031982422\n",
      "Epoch 9, Batch 565, Loss: 2.228205442428589\n",
      "Epoch 9, Batch 570, Loss: 2.2087273597717285\n",
      "Epoch 9, Batch 575, Loss: 2.4796321392059326\n",
      "Epoch 9, Batch 580, Loss: 2.2172393798828125\n",
      "Epoch 9, Batch 585, Loss: 2.407372236251831\n",
      "Epoch 9, Batch 590, Loss: 2.271160364151001\n",
      "Epoch 9, Batch 595, Loss: 2.464695692062378\n",
      "Epoch 9, Batch 600, Loss: 2.238945722579956\n",
      "Epoch 9, Batch 605, Loss: 2.2618207931518555\n",
      "Epoch 9, Batch 610, Loss: 2.3006157875061035\n",
      "Epoch 9, Batch 615, Loss: 2.471524238586426\n",
      "Epoch 9, Batch 620, Loss: 2.503826379776001\n",
      "Epoch 9, Batch 625, Loss: 2.437594413757324\n",
      "Epoch 9, Batch 630, Loss: 2.35715389251709\n",
      "Epoch 9, Batch 635, Loss: 2.4800329208374023\n",
      "Epoch 9, Batch 640, Loss: 2.160452127456665\n",
      "Epoch 9, Batch 645, Loss: 2.270033359527588\n",
      "Epoch 9, Batch 650, Loss: 2.3043723106384277\n",
      "Epoch 9, Batch 655, Loss: 2.5781497955322266\n",
      "Epoch 10, Batch 0, Loss: 2.0913033485412598\n",
      "Epoch 10, Batch 5, Loss: 2.2817418575286865\n",
      "Epoch 10, Batch 10, Loss: 2.1973443031311035\n",
      "Epoch 10, Batch 15, Loss: 2.59250545501709\n",
      "Epoch 10, Batch 20, Loss: 2.234254837036133\n",
      "Epoch 10, Batch 25, Loss: 2.4561874866485596\n",
      "Epoch 10, Batch 30, Loss: 2.222639560699463\n",
      "Epoch 10, Batch 35, Loss: 2.2057878971099854\n",
      "Epoch 10, Batch 40, Loss: 2.3728673458099365\n",
      "Epoch 10, Batch 45, Loss: 2.3326802253723145\n",
      "Epoch 10, Batch 50, Loss: 2.3835299015045166\n",
      "Epoch 10, Batch 55, Loss: 2.384589433670044\n",
      "Epoch 10, Batch 60, Loss: 2.170283794403076\n",
      "Epoch 10, Batch 65, Loss: 2.1943728923797607\n",
      "Epoch 10, Batch 70, Loss: 2.4508113861083984\n",
      "Epoch 10, Batch 75, Loss: 2.3826708793640137\n",
      "Epoch 10, Batch 80, Loss: 2.562243700027466\n",
      "Epoch 10, Batch 85, Loss: 2.3098621368408203\n",
      "Epoch 10, Batch 90, Loss: 2.302198886871338\n",
      "Epoch 10, Batch 95, Loss: 2.395888090133667\n",
      "Epoch 10, Batch 100, Loss: 2.049445152282715\n",
      "Epoch 10, Batch 105, Loss: 2.2782139778137207\n",
      "Epoch 10, Batch 110, Loss: 2.2897870540618896\n",
      "Epoch 10, Batch 115, Loss: 2.3758134841918945\n",
      "Epoch 10, Batch 120, Loss: 2.5264556407928467\n",
      "Epoch 10, Batch 125, Loss: 2.2190961837768555\n",
      "Epoch 10, Batch 130, Loss: 2.3712527751922607\n",
      "Epoch 10, Batch 135, Loss: 2.268845796585083\n",
      "Epoch 10, Batch 140, Loss: 2.435297966003418\n",
      "Epoch 10, Batch 145, Loss: 2.6050961017608643\n",
      "Epoch 10, Batch 150, Loss: 2.4240238666534424\n",
      "Epoch 10, Batch 155, Loss: 2.5698206424713135\n",
      "Epoch 10, Batch 160, Loss: 2.3505654335021973\n",
      "Epoch 10, Batch 165, Loss: 2.199603319168091\n",
      "Epoch 10, Batch 170, Loss: 2.393634080886841\n",
      "Epoch 10, Batch 175, Loss: 2.200669050216675\n",
      "Epoch 10, Batch 180, Loss: 2.2448530197143555\n",
      "Epoch 10, Batch 185, Loss: 2.3007516860961914\n",
      "Epoch 10, Batch 190, Loss: 2.4843153953552246\n",
      "Epoch 10, Batch 195, Loss: 2.3625497817993164\n",
      "Epoch 10, Batch 200, Loss: 2.232714891433716\n",
      "Epoch 10, Batch 205, Loss: 2.364624261856079\n",
      "Epoch 10, Batch 210, Loss: 2.3391921520233154\n",
      "Epoch 10, Batch 215, Loss: 2.3286702632904053\n",
      "Epoch 10, Batch 220, Loss: 2.4203875064849854\n",
      "Epoch 10, Batch 225, Loss: 2.219785690307617\n",
      "Epoch 10, Batch 230, Loss: 2.174375534057617\n",
      "Epoch 10, Batch 235, Loss: 2.3865058422088623\n",
      "Epoch 10, Batch 240, Loss: 2.3030219078063965\n",
      "Epoch 10, Batch 245, Loss: 2.271155595779419\n",
      "Epoch 10, Batch 250, Loss: 2.3338634967803955\n",
      "Epoch 10, Batch 255, Loss: 2.441950798034668\n",
      "Epoch 10, Batch 260, Loss: 2.3355236053466797\n",
      "Epoch 10, Batch 265, Loss: 2.2912752628326416\n",
      "Epoch 10, Batch 270, Loss: 2.3209891319274902\n",
      "Epoch 10, Batch 275, Loss: 2.1403229236602783\n",
      "Epoch 10, Batch 280, Loss: 2.313579797744751\n",
      "Epoch 10, Batch 285, Loss: 2.3373382091522217\n",
      "Epoch 10, Batch 290, Loss: 2.250769853591919\n",
      "Epoch 10, Batch 295, Loss: 2.2711362838745117\n",
      "Epoch 10, Batch 300, Loss: 2.3729190826416016\n",
      "Epoch 10, Batch 305, Loss: 2.2352380752563477\n",
      "Epoch 10, Batch 310, Loss: 2.4362876415252686\n",
      "Epoch 10, Batch 315, Loss: 2.43721604347229\n",
      "Epoch 10, Batch 320, Loss: 2.340315341949463\n",
      "Epoch 10, Batch 325, Loss: 2.3274314403533936\n",
      "Epoch 10, Batch 330, Loss: 2.310917854309082\n",
      "Epoch 10, Batch 335, Loss: 2.340829849243164\n",
      "Epoch 10, Batch 340, Loss: 2.2800018787384033\n",
      "Epoch 10, Batch 345, Loss: 2.2999496459960938\n",
      "Epoch 10, Batch 350, Loss: 2.487044334411621\n",
      "Epoch 10, Batch 355, Loss: 2.2148613929748535\n",
      "Epoch 10, Batch 360, Loss: 2.180590867996216\n",
      "Epoch 10, Batch 365, Loss: 2.3907060623168945\n",
      "Epoch 10, Batch 370, Loss: 2.388406753540039\n",
      "Epoch 10, Batch 375, Loss: 2.4109079837799072\n",
      "Epoch 10, Batch 380, Loss: 2.3626883029937744\n",
      "Epoch 10, Batch 385, Loss: 2.1488165855407715\n",
      "Epoch 10, Batch 390, Loss: 2.5197250843048096\n",
      "Epoch 10, Batch 395, Loss: 2.204364061355591\n",
      "Epoch 10, Batch 400, Loss: 2.5165865421295166\n",
      "Epoch 10, Batch 405, Loss: 2.285210371017456\n",
      "Epoch 10, Batch 410, Loss: 2.2892134189605713\n",
      "Epoch 10, Batch 415, Loss: 2.212489128112793\n",
      "Epoch 10, Batch 420, Loss: 2.6114625930786133\n",
      "Epoch 10, Batch 425, Loss: 2.319624900817871\n",
      "Epoch 10, Batch 430, Loss: 2.3212943077087402\n",
      "Epoch 10, Batch 435, Loss: 2.4994497299194336\n",
      "Epoch 10, Batch 440, Loss: 2.308366298675537\n",
      "Epoch 10, Batch 445, Loss: 2.156223773956299\n",
      "Epoch 10, Batch 450, Loss: 2.2721381187438965\n",
      "Epoch 10, Batch 455, Loss: 2.4421935081481934\n",
      "Epoch 10, Batch 460, Loss: 2.3991637229919434\n",
      "Epoch 10, Batch 465, Loss: 2.567445993423462\n",
      "Epoch 10, Batch 470, Loss: 2.2747535705566406\n",
      "Epoch 10, Batch 475, Loss: 2.361499071121216\n",
      "Epoch 10, Batch 480, Loss: 2.449465036392212\n",
      "Epoch 10, Batch 485, Loss: 2.237567901611328\n",
      "Epoch 10, Batch 490, Loss: 2.3149526119232178\n",
      "Epoch 10, Batch 495, Loss: 2.209257125854492\n",
      "Epoch 10, Batch 500, Loss: 2.6991870403289795\n",
      "Epoch 10, Batch 505, Loss: 2.4459850788116455\n",
      "Epoch 10, Batch 510, Loss: 2.430119037628174\n",
      "Epoch 10, Batch 515, Loss: 2.3868300914764404\n",
      "Epoch 10, Batch 520, Loss: 2.239231824874878\n",
      "Epoch 10, Batch 525, Loss: 2.2790417671203613\n",
      "Epoch 10, Batch 530, Loss: 2.2174129486083984\n",
      "Epoch 10, Batch 535, Loss: 2.1628496646881104\n",
      "Epoch 10, Batch 540, Loss: 2.5461957454681396\n",
      "Epoch 10, Batch 545, Loss: 2.359684944152832\n",
      "Epoch 10, Batch 550, Loss: 2.334881067276001\n",
      "Epoch 10, Batch 555, Loss: 2.2239809036254883\n",
      "Epoch 10, Batch 560, Loss: 2.3470075130462646\n",
      "Epoch 10, Batch 565, Loss: 2.4379115104675293\n",
      "Epoch 10, Batch 570, Loss: 2.4305598735809326\n",
      "Epoch 10, Batch 575, Loss: 2.3595685958862305\n",
      "Epoch 10, Batch 580, Loss: 2.4647328853607178\n",
      "Epoch 10, Batch 585, Loss: 2.2213311195373535\n",
      "Epoch 10, Batch 590, Loss: 2.3760805130004883\n",
      "Epoch 10, Batch 595, Loss: 2.1999197006225586\n",
      "Epoch 10, Batch 600, Loss: 2.3146188259124756\n",
      "Epoch 10, Batch 605, Loss: 2.211613655090332\n",
      "Epoch 10, Batch 610, Loss: 2.532635450363159\n",
      "Epoch 10, Batch 615, Loss: 2.5161752700805664\n",
      "Epoch 10, Batch 620, Loss: 2.4927971363067627\n",
      "Epoch 10, Batch 625, Loss: 2.197589159011841\n",
      "Epoch 10, Batch 630, Loss: 2.4170966148376465\n",
      "Epoch 10, Batch 635, Loss: 2.488816499710083\n",
      "Epoch 10, Batch 640, Loss: 2.3812365531921387\n",
      "Epoch 10, Batch 645, Loss: 2.5202248096466064\n",
      "Epoch 10, Batch 650, Loss: 2.4422945976257324\n",
      "Epoch 10, Batch 655, Loss: 2.312143564224243\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.02\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(xs.size(0))\n",
    "    for i in range(0, xs.size(0), batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_xs, batch_ys = xs[indices], ys[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_xs)\n",
    "        loss = F.cross_entropy(logits, batch_ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % (5 * batch_size) == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i//batch_size}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558dc900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zm\n",
      "mes\n",
      "stholy\n",
      "tascamamp\n",
      "wh\n",
      "shibily\n",
      "karufledinilitiog\n",
      "jiomstriscliatiptind\n",
      "kizm\n",
      "fr\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_word(model, stoi, itos):\n",
    "\n",
    "    current_chars = ['.'] + random.sample(chars, 1)\n",
    "\n",
    "    while True:  \n",
    "        input_ix = [stoi[current_chars[-2]], stoi[current_chars[-1]]]\n",
    "        input_tensor = torch.tensor([input_ix], dtype=torch.long)\n",
    "\n",
    "        logits = model(input_tensor)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_char_ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        next_char = itos[next_char_ix]\n",
    "\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        current_chars.append(next_char)\n",
    "\n",
    "    return ''.join(current_chars[1:])\n",
    "\n",
    "for _ in range(10):\n",
    "    word = generate_word(model, stoi, itos)\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e8a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
